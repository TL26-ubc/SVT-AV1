This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
src/
  bridge/
    cb_registration.cpp
    cb_registration.hpp
    pybridge.cpp
    pybridge.h
    utils.cpp
    utils.hpp
  pyencoder/
    environment/
      __init__.py
      av1_runner.py
      constants.py
      naive_env.py
      train_refactor.py
    states/
      abstract.py
      naive.py
    utils/
      video_reader.py
    __init__.py
    _binding.cpp
CMakeLists.txt
monitor.monitor.csv
pyproject.toml
test.py

================================================================
Files
================================================================

================
File: src/bridge/cb_registration.cpp
================
#include "cb_registration.hpp"
#include "utils.hpp"
#include "pybridge.h"

namespace py = pybind11;

namespace pybridge {

Callback* g_callbacks[static_cast<int>(CallbackEnum::Count)] = {nullptr};

static int set_cb_ptr(CallbackEnum which, bool unset)
{
    switch (which) {
        case CallbackEnum::GetDeltaQOffset:
            get_deltaq_offset_cb = unset ? nullptr : get_deltaq_offset_trampoline;
            return 0;
        case CallbackEnum::RecvPictureFeedback:
            recv_picture_feedback_cb = unset ? nullptr : recv_picture_feedback_trampoline;
            return 0;
        default:
            return -1;
    }
}

int pybridge_set_cb(CallbackEnum which, py::object callable)
{
    Callback &slot = *g_callbacks[static_cast<int>(which)];

    if (callable.is_none()) {
        // Unset function
        slot.py_func = std::move(py::none());
    }
    else {
        // Validate and store new function
        py::function function =  pyutils::validate_callable(callable, slot.n_args);
        slot.py_func = std::move(function);
    }

    // Enable the C trampoline for the encoder
    return set_cb_ptr(which, callable.is_none());
}

} // namespace bridge

================
File: src/bridge/cb_registration.hpp
================
#pragma once
#include <pybind11/pybind11.h>

namespace pybridge {

enum class CallbackEnum {
    GetDeltaQOffset,
    RecvPictureFeedback,
    Count
};

struct Callback {
    pybind11::object py_func;
    void            *c_trampoline;
    int              n_args;
};

extern Callback* g_callbacks[static_cast<int>(CallbackEnum::Count)];

int pybridge_set_cb(CallbackEnum which, pybind11::object callable);

} // namespace pybridge

================
File: src/bridge/pybridge.cpp
================
#include "cb_registration.hpp"
#include "utils.hpp"
#include "pybridge.h"

#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

namespace py = pybind11;
using namespace pybridge;

get_deltaq_offset_cb_t     get_deltaq_offset_cb     = nullptr;
recv_picture_feedback_cb_t recv_picture_feedback_cb = nullptr;

extern "C" void get_deltaq_offset_trampoline(SuperBlockInfo *sb_info_array, int *offset_array, uint32_t sb_count, int32_t frame_type, int32_t picture_number) {
    Callback &cb = *g_callbacks[static_cast<int>(CallbackEnum::GetDeltaQOffset)];
    if (cb.py_func.is_none())
        return;

    py::gil_scoped_acquire acquire;

    py::function fcn = pyutils::validate_callable(cb.py_func, cb.n_args);

    // Convert sb_infos to dictionary
    py::list sb_info_list = pyutils::to_pylist(sb_info_array, sb_count, [](const SuperBlockInfo &sb) {
        return py::dict(
            py::arg("sb_org_x") = sb.sb_org_x,
            py::arg("sb_org_y") = sb.sb_org_y,
            py::arg("sb_height") = sb.sb_height,
            py::arg("sb_width") = sb.sb_width,
            py::arg("sb_qindex") = sb.sb_qindex,
            py::arg("sb_x_mv") = sb.sb_x_mv,
            py::arg("sb_y_mv") = sb.sb_y_mv
        );
    });

    py::object ret = fcn.operator()(sb_info_list, frame_type, picture_number);

    if (!py::isinstance<py::list>(ret)) {
        throw py::type_error("Expected return value of type list however was " +
                             py::cast<std::string>(ret.get_type().attr("__name__")));
    }

    py::list qp_map = py::cast<py::list>(ret);
    if (qp_map.size() != sb_count) {
        throw py::value_error("Expected return value of type list with size " + std::to_string(sb_count) +
                              " however was of size " + std::to_string(qp_map.size()));
    }

    for (uint32_t i = 0; i < sb_count; ++i) {
        py::object item = qp_map[i];

        if (!py::isinstance<py::int_>(item)) {
            throw py::type_error("qp_map[" + std::to_string(i) + "] is not an int, got " +
                                 py::cast<std::string>(item.get_type().attr("__name__")));
        }

        offset_array[i] = item.cast<int>();
    }
}

extern "C" void recv_picture_feedback_trampoline(uint8_t *bitstream, uint32_t bitstream_size, uint32_t picture_number) {
    Callback &cb = *g_callbacks[static_cast<int>(CallbackEnum::RecvPictureFeedback)];
    if (cb.py_func.is_none())
        return;

    py::gil_scoped_acquire acquire;

    py::function fcn = pyutils::validate_callable(cb.py_func, cb.n_args);

    py::bytes py_bitstream = py::bytes(reinterpret_cast<char *>(bitstream), bitstream_size);

    fcn.operator()(py_bitstream, bitstream_size, picture_number);
}

================
File: src/bridge/pybridge.h
================
#ifndef PYBRIDGE_H_
#define PYBRIDGE_H_

#ifdef __cplusplus
extern "C" {
#endif

#include <stdint.h>
#include "../../../Source/API/EbSvtAv1Enc.h"

typedef void (*get_deltaq_offset_cb_t)(SuperBlockInfo *, int *, uint32_t, int32_t, int32_t);

typedef void (*recv_picture_feedback_cb_t)(uint8_t *, uint32_t, uint32_t);

extern get_deltaq_offset_cb_t get_deltaq_offset_cb;
extern recv_picture_feedback_cb_t recv_picture_feedback_cb;

void get_deltaq_offset_trampoline(SuperBlockInfo *, int *, uint32_t, int32_t, int32_t);

void recv_picture_feedback_trampoline(uint8_t *, uint32_t, uint32_t);

#ifdef __cplusplus
} /* extern "C" */
#endif
#endif /* PYBRIDGE_H_ */

================
File: src/bridge/utils.cpp
================
#include "utils.hpp"

#include <pybind11/pybind11.h>
#include <pybind11/functional.h>
#include <Python.h>

#include <cassert>

namespace pyutils {

py::function validate_callable(const py::object &callable, int expected_n_args)
{
    assert(PyGILState_Check());

    if (!PyCallable_Check(callable.ptr())) {
        throw py::type_error("object must be callable");
    }

    // verify __code__ object
    py::object code_obj = callable.attr("__code__");
    if (!PyCode_Check(code_obj.ptr())) {
        throw py::type_error("callable has no valid __code__");
    }

    // Grab arg numbers
    PyCodeObject *co = reinterpret_cast<PyCodeObject *>(code_obj.ptr());
    const int nargs       = co->co_argcount + co->co_posonlyargcount;
    const bool has_vararg = (co->co_flags & CO_VARARGS)     != 0;
    const bool has_varkw  = (co->co_flags & CO_VARKEYWORDS) != 0;

    if (has_vararg || has_varkw) {
        throw py::type_error("callback may not use *args or **kwargs");
    }

    if (nargs != expected_n_args) {
        throw py::type_error(
            "callback must take exactly " + std::to_string(expected_n_args) + " positional arguments (got " + std::to_string(nargs) + ")"
        );
    }

    // Return as a function
    return py::reinterpret_borrow<py::function>(callable);
}

}

================
File: src/bridge/utils.hpp
================
#pragma once
#include <pybind11/pybind11.h>

namespace py  = pybind11;

namespace pyutils {

py::function validate_callable(const py::object &callable, int expected_n_args);

template <typename T, typename Transform>
py::list to_pylist(const T *data, std::size_t n, Transform &&fn)
{
    if (!data)
        throw py::value_error("data pointer is null");

    py::list out(n);
    for (std::size_t i = 0; i < n; ++i)
        out[i] = fn(data[i]);
    return out;
}

} // namespace pybridge

================
File: src/pyencoder/environment/__init__.py
================
from gymnasium.envs.registration import register

register(
    id="Av1Env-v0",
    entry_point="src.environment:Av1Env"
)

================
File: src/pyencoder/environment/av1_runner.py
================
import io
from typing import Any, Dict, List, Optional, TypedDict
from queue import Queue
from pathlib import Path
from pyencoder import SuperBlockInfo
import threading
from dataclasses import dataclass

import av
import cv2
import pyencoder

@dataclass
class Observation:
    superblocks: list[SuperBlockInfo]
    frame_type: int
    picture_number: int

@dataclass
class Action:
    skip: bool
    offsets: list[int]

global the_only_object
the_only_object = None

def picture_feedback_trampoline(bitstream: bytes, size: int, picture_number: int):
    the_only_object.picture_feedback(bitstream, size, picture_number)

def get_deltaq_offset_trampoline(sbs: list[SuperBlockInfo], frame_type: int, picture_number: int) -> list[int]:
    return the_only_object.get_deltaq_offset(sbs, frame_type, picture_number)

class Av1Runner:
    """
    A class to handle callbacks from Python to C for encoding video frames.
    This class is designed to be used with a C encoder that requires specific
    callbacks for frame processing.
    """

    def __init__(self, video_path):
        global the_only_object
        if the_only_object is not None:
            raise RuntimeError(
                "EncoderCallback instance already exists. Only one instance is allowed."
            )
        the_only_object = self

        self.video_path = video_path
        self.bytes_keeper = {}
        self.previous_training_bytes_keeper = {}  # Store previous training bytes
        self.all_bitstreams = io.BytesIO()  # Holds joined bitstream data

        w, h = frame_dims_from_file(video_path)
        self.sb_total_count = superblocks_from_dims(w, h)

        self.first_round = True  # Flag for the first round of encoding
        self.first_round_byte_usage = {}  # Store byte usage for the first round

        # Synchronization
        self.observation_queue: Queue[Observation] = Queue(maxsize=1) # Encoder provides observation
        self.action_queue: Queue[Action] = Queue(maxsize=1) # RL provides action
        self.feedback_queue: Queue = Queue(maxsize=10) # Encoder provides feedback to RL
        self.encoder_thread: threading.Thread = None # Encoding thread

    def run(self, output_path: str = None, block: bool = False):
        """
        Start the encoder in a new thread.
        If block is True, wait for the encoder to finish.
        """
        if self.encoder_thread and self.encoder_thread.is_alive():
            print("Waiting for previous encoder thread to terminate...")
            self.encoder_thread.join(timeout=20.0)

        self.encoder_thread = threading.Thread(
            target=self._run_encoder,
            args=(output_path,),
            daemon=True,
            name="EncoderThread"
        )
        self.encoder_thread.start()
        
        if block:
            self.encoder_thread.join()

    def _run_encoder(self, output_path: str = None):
        print("Starting encoder thread...")
        self.reset()
        self.register_callbacks()

        args = {
            "input": self.video_path,
            "pred_struct": 1,
            "rc": 2,
            "tbr": 100,
            "enable_stat_report": True,
        }

        if output_path:
            args["b"] = output_path

        pyencoder.run(**args)

    def join(self):
        if not self.encoder_thread or not self.encoder_thread.is_alive():
            return
        else:
            self.encoder_thread.join()

    def register_callbacks(self):
        pyencoder.register_callbacks(
            get_deltaq_offset=get_deltaq_offset_trampoline,
            picture_feedback=picture_feedback_trampoline,
        )

    def reset(self):
        """
        Reset the callback state, clearing the bytes_keeper and all_bitstreams.
        This is typically called at the start of a new encoding session.
        """
        self.previous_training_bytes_keeper = self.bytes_keeper.copy()
        self.bytes_keeper.clear()
        self.all_bitstreams.close()
        self.all_bitstreams = io.BytesIO()

        # Clear queues
        while not self.observation_queue.empty():
            self.observation_queue.get_nowait()

        while not self.action_queue.empty():
            self.action_queue.get_nowait()

        while not self.feedback_queue.empty():
            self.feedback_queue.get_nowait()

    def join_bitstreams(self):
        while joined_bitstream_num in self.bytes_keeper.keys():
            self.all_bitstreams += self.bytes_keeper[joined_bitstream_num]
            joined_bitstream_num += 1

    def picture_feedback(self, bitstream: bytes, size: int, picture_number: int):
        """
        Callback for receiving encoded picture data from the C encoder.
        This sends feedback to the RL environment.
        """
        self.bytes_keeper[picture_number] = bitstream
        encoded_frame_data = self.get_last_frame(bitstream=bitstream)

        # Prepare feedback for RL environment
        feedback_data = {
            "picture_number": picture_number,
            "bitstream_size": size,
            "encoded_frame_data": encoded_frame_data,
        }

        # Send feedback to RL environment (non-blocking)
        self.feedback_queue.put_nowait(feedback_data)

    def get_last_frame(self, bitstream):
        byte_file = self.all_bitstreams
        byte_file.write(bitstream)
        byte_file.seek(0)
        container = av.open(byte_file)
        last_frame = None
        for frame in container.decode(video=0):
            last_frame = frame

        assert last_frame != None
        img_array = last_frame.to_ndarray(format="rgb24")
        ycrcb_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2YCrCb)

        # if the last frame is a keyframe, we can write the bitstream to the file
        if last_frame.key_frame:
            byte_file.close()
            self.all_bitstreams = io.BytesIO()  # get a new bytefile
            self.all_bitstreams.write(bitstream)  # write the keyframe to bytefile
        container.close()
        return ycrcb_array

    def get_deltaq_offset(self, sbs: list[SuperBlockInfo], frame_type: int, picture_number: int) -> list[int]:
        """
        Callback to get QP offsets for superblocks in a frame.
        This method MUST return immediately as the encoder waits synchronously.
        """
        # Request action from RL environment
        observation = Observation(
            picture_number=picture_number,
            superblocks=sbs,
            frame_type=frame_type,
        )

        # Send action request to RL environment (blocking call)
        self.observation_queue.put_nowait(observation)

        # Wait for RL response
        action = self.action_queue.get()

        # Dummy offsets to skip
        if action.skip:
            return [114514] * self.sb_total_count

        if len(action.offsets) != self.sb_total_count:
            raise ValueError(f"Action response length mismatch. Expected {self.sb_total_count}, got {len(action_response)}")

        return action.offsets

    def wait_for_next_observation(self) -> Observation:
        """
        Wait for action request from encoder.
        Called by RL environment to get the next frame to process.
        """
        return self.observation_queue.get()

    def send_action_response(self, *, action: List[int] = None, skip = False):
        """
        Send action response to encoder.
        Called by RL environment to provide QP offsets.
        """
        self.action_queue.put(Action(
            skip=skip,
            offsets=action
        ))

    def wait_for_feedback(self) -> Optional[Dict]:
        """
        Wait for feedback from encoder.
        Called by RL environment to get encoding results.
        """
        return self.feedback_queue.get()

    def get_byte_usage_diff(self, picture_number: int) -> tuple[int, int]:
        """
        Get the byte usage difference for a specific frame compared to the first round.
        Returns the difference in bytes used for encoding the frame.
        Args:
            picture_number (int): The frame number to check.
        Returns:
            (int, int): A tuple containing the difference in bytes and the current size of the frame.
            positive difference means the frame is larger than in the first round,
            negative difference means the frame is smaller.
        """
        if (
            picture_number in self.first_round_byte_usage
            and picture_number in self.bytes_keeper
        ):
            first_round_size = self.first_round_byte_usage[picture_number]
            current_size = len(self.bytes_keeper.get(picture_number, b""))
            return (first_round_size - current_size, current_size)
        assert False, f"Frame {picture_number} not found in first round byte usage"
        
    def save_bitstream_to_file(self, output_path: str, interrupt: bool = False):
        """
        Save the bitstream to a file.
        If not interrupted, it will save current bitstream data to ivf file.
        If interrupted, it will save the previous training bytes to ivf file.
        
        If the desierd bitestream is not complete, nothing will be saved and a warning will be given
        Args:
            output_path (str): The path to save the bitstream file. Must end with .ivf.
            interrupt (bool): If True, save the previous training bytes instead of current.
        Raises:
            ValueError: If the output path does not end with .ivf.
        """
        if not output_path.endswith(".ivf"):
            raise ValueError("Output path must end with .ivf")


        # Save previous training bytes
        # To make the bitstream playable, prepend the IVF header and frame headers
        frames = list(self.previous_training_bytes_keeper.values())
        if not frames:
            bitstream_data = b""
        else:
            width = 0
            height = 0
            # Try to get width/height from the first frame using PyAV
            container = av.open(io.BytesIO(frames[0]))
            video_stream = next(s for s in container.streams if s.type == "video")
            width = video_stream.width
            height = video_stream.height
            container.close()
            bitstream_data = ivf_header(len(frames), width, height)
            for i, frame in enumerate(frames):
                bitstream_data += ivf_frame_header(frame, i)
                bitstream_data += frame


        if not bitstream_data:
            print("No bitstream data to save.")
            return

        with open(output_path, "wb") as f:
            f.write(bitstream_data)
        print(f"Bitstream saved to {output_path}")

def ivf_header(num_frames, width, height, fourcc=b'AV01'):
    header = b'DKIF'  # signature
    header += (0).to_bytes(2, 'little')  # version
    header += (32).to_bytes(2, 'little')  # header size
    header += fourcc  # fourcc
    header += width.to_bytes(2, 'little')
    header += height.to_bytes(2, 'little')
    header += (30).to_bytes(4, 'little')  # timebase denominator
    header += (1).to_bytes(4, 'little')   # timebase numerator
    header += num_frames.to_bytes(4, 'little')
    header += (0).to_bytes(4, 'little')  # unused
    return header

def ivf_frame_header(frame_bytes, pts):
    return len(frame_bytes).to_bytes(4, 'little') + pts.to_bytes(8, 'little')

def superblocks_from_dims(width: int, height: int, block_size: int = 64) -> int:
    """
    Return the number of super-blocks (block_size × block_size tiles)
    that cover a frame of size (width × height).

    Uses ceiling division so partially-covered edges count as full blocks.
    """
    if block_size <= 0:
        raise ValueError("block_size must be positive")

    blocks_w = (width  + block_size - 1) // block_size
    blocks_h = (height + block_size - 1) // block_size
    return blocks_w * blocks_h

def frame_dims_from_capture(cap: cv2.VideoCapture) -> tuple[int, int]:
    """
    Grab the first frame from an *open* cv2.VideoCapture and
    return (width, height).  Restores the original frame pointer.
    """
    if not cap.isOpened():
        raise ValueError("cv2.VideoCapture is not opened")

    pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
    ok, frame = cap.read()
    cap.set(cv2.CAP_PROP_POS_FRAMES, pos) # restore

    if not ok:
        raise ValueError("Could not read a frame from capture")

    height, width = frame.shape[:2]
    return width, height

def frame_dims_from_file(path: str | Path) -> tuple[int, int]:
    """
    Open the video at *path*, read its first frame, and return (width, height).
    Closes the capture automatically.
    """
    cap = cv2.VideoCapture(str(path))
    try:
        if not cap.isOpened():
            raise FileNotFoundError(f"Could not open video file: {path}")
        return frame_dims_from_capture(cap)
    finally:
        cap.release()

================
File: src/pyencoder/environment/constants.py
================
QP_MIN, QP_MAX = -3, 3  # delta QP range which will be action
SB_SIZE = 64 # superblock size
BIT_DEPTH = 255

================
File: src/pyencoder/environment/naive_env.py
================
import threading
from pathlib import Path
from typing import Any, Dict, Optional, Tuple
import av
import cv2
import gymnasium as gym
import numpy as np
from pyencoder.environment.av1_runner import Av1Runner
from pyencoder.utils.video_reader import VideoReader
from pyencoder.environment.constants import SB_SIZE, QP_MIN, QP_MAX
import math
from pyencoder.states.abstract import AbstractState
from pyencoder.states.naive import NaiveState

# Extending gymnasium's Env class
# https://gymnasium.farama.org/api/env/#gymnasium.Env
class Av1GymEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(
        self,
        video_path: str | Path,
        output_dir: str | Path,
        *,
        lambda_rd: float = 0.1,
        queue_timeout=10,
        state: type[AbstractState] = NaiveState,
    ):
        super().__init__()
        self.video_path = Path(video_path)
        self.output_dir = Path(output_dir)
        self.av1_runner = Av1Runner(video_path)

        # Initialize the VideoReader
        self.video_reader = VideoReader(path=video_path)

        self.lambda_rd = lambda_rd
        self._episode_done = threading.Event()

        self.num_superblocks = self.video_reader.get_num_superblock()
        self.num_frames = self.video_reader.get_frame_count()

        # Action space = QP offset grid
        self.action_space = gym.spaces.MultiDiscrete(
            # num \in [QP_MAX, QP_MIN], there are num_superblocks of them
            [QP_MAX - QP_MIN + 1]
            * self.num_superblocks
        )

        # RL/encoder communication
        self.queue_timeout = queue_timeout

        # Episode management
        self.current_frame = 0
        self.terminated = False

        # Frame data storage
        self.frame_history = []

        # Synchronization
        self.encoder_thread = None

        # run the first round of encoding, save the baseline video at specified output path
        self.av1_runner.run(
            output_path=f"{str(output_dir)}/baseline_output.ivf"
        )

        # Get baseline observations
        baseline_obs = []
        for _ in range(self.num_frames):
            obs = self.av1_runner.wait_for_next_observation()
            baseline_obs.append(obs)
            self.av1_runner.send_action_response(skip=True)
            _ = self.av1_runner.wait_for_feedback()

        # ensure thread has finished
        self.av1_runner.join()

        self.state_wrapper = state(video_reader=self.video_reader, baseline_observations=baseline_obs, sb_size=SB_SIZE)

        self.observation_space = self.state_wrapper.get_observation_space()

        self.y_psnr_list = []
        self.cb_psnr_list = []
        self.cr_psnr_list = []
        self.baseline_heighest_psnr = {
            "y": -114514.0,  # Initialize with very low values
            "cb": -114514.0,
            "cr": -114514.0,
        }
        
        # Save baseline frame PSNRs
        self.save_baseline_frame_psnr(
            baseline_video_path=f"{str(output_dir)}/baseline_output.ivf"
        )
        
    def save_baseline_frame_psnr(self, baseline_video_path: str | Path):
        """Calculate and save PSNR for baseline frames"""
        baseline_video_path = str(baseline_video_path)
        container = av.open(baseline_video_path)
        stream = container.streams.video[0]

        total_frames = int(stream.frames)
        assert (
            total_frames == self.num_frames
        ), f"Baseline video frame count {total_frames} does not match expected {self.num_frames}"

        assert (
            self.y_psnr_list == []
        ), "PSNR lists should be empty before saving baseline PSNRs"
        assert (
            self.cb_psnr_list == []
        ), "PSNR lists should be empty before saving baseline PSNRs"
        assert (
            self.cr_psnr_list == []
        ), "PSNR lists should be empty before saving baseline PSNRs"

        for frame_number, frame in enumerate(container.decode(stream)):
            # Convert frame to YCbCr and get numpy arrays for Y, Cb, Cr
            ycbcr = frame.to_ndarray(format="rgb24")
            ycbcr = cv2.cvtColor(ycbcr, cv2.COLOR_RGB2YCrCb)

            # Get YCbCr PSNR for the current frame
            y_psnr, cb_psnr, cr_psnr = self.video_reader.ycrcb_psnr(frame_number, ycbcr, self.baseline_heighest_psnr)            
            # Validate PSNR values
            if not np.all(np.isfinite([y_psnr, cb_psnr, cr_psnr])):
                invalid_names = [name for val, name in zip([y_psnr, cb_psnr, cr_psnr], ["Y", "Cb", "Cr"]) if not np.isfinite(val)]
                print(f"Warning: Invalid PSNR(s) {invalid_names} for baseline frame {frame_number}")
                raise InvalidStateError(
                    f"Invalid PSNR(s) {invalid_names} for baseline frame {frame_number}"
                )
            
            # Append to lists
            self.y_psnr_list.append(y_psnr)
            self.cb_psnr_list.append(cb_psnr)
            self.cr_psnr_list.append(cr_psnr)

        assert (
            len(self.y_psnr_list) == self.num_frames
        ), f"Expected {self.num_frames} Y PSNR values, got {len(self.y_psnr_list)}"
        assert (
            len(self.cb_psnr_list) == self.num_frames
        ), f"Expected {self.num_frames} Cb PSNR values, got {len(self.cb_psnr_list)}"
        assert (
            len(self.cr_psnr_list) == self.num_frames
        ), f"Expected {self.num_frames} Cr PSNR values, got {len(self.cr_psnr_list)}"
        container.close()
        
        self.baseline_heighest_psnr['y'] = max(self.y_psnr_list)
        self.baseline_heighest_psnr['cb'] = max(self.cb_psnr_list)
        self.baseline_heighest_psnr['cr'] = max(self.cr_psnr_list)
        
        # iterate through each list, replace negative values with heighest PSNR
        for i in range(self.num_frames):
            if self.y_psnr_list[i] < 0:
                self.y_psnr_list[i] = self.baseline_heighest_psnr['y']
            if self.cb_psnr_list[i] < 0:
                self.cb_psnr_list[i] = self.baseline_heighest_psnr['cb']
            if self.cr_psnr_list[i] < 0:
                self.cr_psnr_list[i] = self.baseline_heighest_psnr['cr']
        
        print("Baseline frame PSNRs saved:")

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.reset
    def reset(
        self, *, seed: int | None = None, options: dict | None = None
    ) -> Tuple[dict, dict]:
        print("Resetting environment...")

        super().reset(seed=seed)

        # Reset episode state
        self.current_frame = 0
        self.current_episode_reward = 0.0
        self.terminated = False
        self.frame_history.clear()

        # Start encoder in separate thread
        self.av1_runner.run()

        # Get initial observation
        initial_obs = self._get_next_observation()

        info = {
            "frame_number": self.current_frame,
            "reward": 0,
            "bitstream_size": 0,
            "episode_frames": self.current_frame,
        }

        return initial_obs, info

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        if self.terminated:
            raise RuntimeError("Episode has ended. Call reset() before step().")

        # Validate action
        if action.shape != (self.num_superblocks,):
            raise ValueError(
                f"Action shape {action.shape} != expected ({self.num_superblocks},)"
            )
          
        # Validate action values
        validate_array(action, f"Action for frame {self.current_frame}")
        
        # Convert action to QP offsets
        qp_offsets = self._action_to_qp_offsets(action)
        
        # Send action response to encoder
        self.av1_runner.send_action_response(action=qp_offsets.tolist())
        
        # Wait for encoding feedback
        feedback = self.av1_runner.wait_for_feedback()

        # Calculate reward
        reward = self._calculate_reward(feedback, qp_offsets)
        self.current_episode_reward += reward

        # Update episode state
        self.current_frame += 1

        # Check termination conditions
        self._check_termination_conditions()
        
        # Get next observation with validation
        if self.terminated:
            # Episode has ended, return dummy observation
            next_obs = np.zeros(self.observation_space.shape, dtype=np.float32)
            print(f"Episode terminated at frame {self.current_frame-1} (total frames: {self.num_frames})")
        else:
            # Get next observation with validation
            try:
                next_obs = self._get_next_observation()
            except Exception as e:
                print(f"Failed to get observation for frame {self.current_frame}: {e}")
                # Force termination and return dummy observation
                self.terminated = True
                next_obs = np.zeros(self.observation_space.shape, dtype=np.float32)
        
        # Store frame data
        self.frame_history.append(
            {
                "frame_number": self.current_frame,
                "qp_offsets": qp_offsets,
                "reward": reward,
                "feedback": feedback,
            }
        )

        info = {
            "frame_number": self.current_frame,
            "reward": reward,
            "bitstream_size": feedback.get("bitstream_size", 0),
            "episode_frames": self.current_frame,
        }

        return next_obs, reward, self.terminated, False, info

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.close
    def close(self):
        print("Closing environment...")

        if self.encoder_thread and self.encoder_thread.is_alive():
            self._episode_done.set()
            self.encoder_thread.join(timeout=2.0)
        
        print("Environment closed")

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.render
    def render(self):
        pass
    
    def save_bitstream_to_file(self, output_path: str, interrupt: bool = False):
        """Save the bitstream to a file"""
        self.av1_runner.save_bitstream_to_file(output_path, interrupt=interrupt)

    def _get_next_observation(self) -> tuple[np.ndarray]:
        """Get current observation based on current frame"""
        try:
            observation = self.av1_runner.wait_for_next_observation()
            current_frame = self.video_reader.read_frame(frame_number=observation.picture_number)

            # Assert frame numbers match
            assert (
                observation.picture_number == self.current_frame
            ), f"observation frame {observation.picture_number} != current_frame {self.current_frame}"

            frame_state = self.state_wrapper.get_observation(
                current_frame, 
                observation.superblocks, 
                observation.frame_type, 
                observation.picture_number
            )
            
            # Validate the observation
            validate_array(frame_state, f"Current observation (frame {self.current_frame})")
            
            return frame_state
            
        except Exception as e:
            raise InvalidStateError(f"Failed to get current observation for frame {self.current_frame}: {e}")

    def _action_to_qp_offsets(self, action: np.ndarray) -> np.ndarray:
        """Convert discrete action to QP offsets"""
        # Map from [0, QP_MAX-QP_MIN] to [QP_MIN, QP_MAX]
        qp_offsets = action + QP_MIN
        qp_offsets = np.clip(qp_offsets, QP_MIN, QP_MAX)
        
        # Validate QP offsets
        validate_array(qp_offsets, f"QP offsets for frame {self.current_frame}")
        
        return qp_offsets

    def _calculate_reward(
        self, feedback: Dict, qp_offsets: np.ndarray
    ) -> float:
        """Calculate reward based on encoding feedback"""
        try:
            # Get encoded frame data
            encoded_frame_data = feedback["encoded_frame_data"]
            
            # Get YCbCr PSNR for the current frame
            y_psnr, cb_psnr, cr_psnr = self.video_reader.ycrcb_psnr(
                self.current_frame, encoded_frame_data, self.baseline_heighest_psnr
            )
            
            # Validate PSNR values
            # for psnr_val, psnr_name in [(y_psnr, "Y"), (cb_psnr, "Cb"), (cr_psnr, "Cr")]:
            #     if math.isnan(psnr_val) or math.isinf(psnr_val):
            #         raise InvalidRewardError(
            #             f"Invalid {psnr_name} PSNR ({psnr_val}) for frame {self.current_frame}"
            #         )
            
            # Get byte usage difference
            # byte_saved, current_usage = self.av1_runner.get_byte_usage_diff(
            #     action_request["picture_number"]
            # )
            
            # Validate byte usage values
            # if math.isnan(byte_saved) or math.isinf(byte_saved):
            #     raise InvalidRewardError(f"Invalid byte_saved ({byte_saved}) for frame {self.current_frame}")
            # if math.isnan(current_usage) or math.isinf(current_usage) or current_usage <= 0:
            #     raise InvalidRewardError(f"Invalid current_usage ({current_usage}) for frame {self.current_frame}")
            
            # Calculate PSNR improvements
            y_psnr_improvement = y_psnr - self.y_psnr_list[self.current_frame]
            # cb_psnr_improvement = cb_psnr - self.cb_psnr_list[self.current_frame]
            # cr_psnr_improvement = cr_psnr - self.cr_psnr_list[self.current_frame]
            
            # Validate improvements
            # for improvement, name in [
            #     (y_psnr_improvement, "Y PSNR improvement"),
            #     (cb_psnr_improvement, "Cb PSNR improvement"), 
            #     (cr_psnr_improvement, "Cr PSNR improvement")
            # ]:
            #     if math.isnan(improvement) or math.isinf(improvement):
            #         raise InvalidRewardError(f"Invalid {name} ({improvement}) for frame {self.current_frame}")
            
            # Calculate reward components
            a = 1
            # b = c = 0.5
            # d = 2
            
            # byte_efficiency = byte_saved / current_usage
            
            # Validate byte efficiency
            # if math.isnan(byte_efficiency) or math.isinf(byte_efficiency):
            #     raise InvalidRewardError(
            #         f"Invalid byte efficiency ({byte_efficiency}) for frame {self.current_frame}. "
            #         f"byte_saved: {byte_saved}, current_usage: {current_usage}"
            #     )
            
            # Calculate final reward
            reward = (
                y_psnr_improvement * a
                # + cb_psnr_improvement * b
                # + cr_psnr_improvement * c
                # + byte_efficiency * d
            )
            
            # Final reward validation
            # reward_details = {
            #     "y_psnr": y_psnr,
            #     "cb_psnr": cb_psnr,
            #     "cr_psnr": cr_psnr,
            #     "y_psnr_improvement": y_psnr_improvement,
            #     "cb_psnr_improvement": cb_psnr_improvement,
            #     "cr_psnr_improvement": cr_psnr_improvement,
            #     "byte_saved": byte_saved,
            #     "current_usage": current_usage,
            #     "byte_efficiency": byte_efficiency
            # }
            
            # validate_reward(reward, self.current_frame, reward_details)
            
            return reward
            
        except Exception as e:
            raise InvalidRewardError(f"Failed to calculate reward for frame {self.current_frame}: {e}")

    def _check_termination_conditions(self):
        """Check if episode should terminate"""
        # Maximum frames reached
        if self.current_frame >= self.num_frames:
            self.terminated = True

class InvalidStateError(Exception):
    pass


class InvalidRewardError(Exception):
    pass


def validate_array(arr: np.ndarray, name: str) -> None:
    """Validate numpy array for NaN, inf, and other invalid values"""
    if arr is None:
        raise ValueError(f"{name} is None")

    if not isinstance(arr, np.ndarray):
        raise TypeError(f"{name} must be numpy array, got {type(arr)}")

    if arr.size == 0:
        raise ValueError(f"{name} is empty")

    # Check for NaN values
    nan_mask = np.isnan(arr)
    if np.any(nan_mask):
        nan_count = np.sum(nan_mask)
        nan_indices = np.where(nan_mask)
        raise InvalidStateError(
            (
                (
                    f"{name} contains {nan_count} NaN values at indices: {nan_indices}. "
                    f"Array shape: {arr.shape}, dtype: {arr.dtype}\n"
                    f"Array sample: {arr.flat[:min(10, arr.size)]}"
                )
            )
        )

    # Check for infinite values
    inf_mask = np.isinf(arr)
    if np.any(inf_mask):
        inf_count = np.sum(inf_mask)
        inf_indices = np.where(inf_mask)
        raise InvalidStateError(
            f"{name} contains {inf_count} infinite values at indices: {inf_indices}. "
            f"Array shape: {arr.shape}, dtype: {arr.dtype}\n"
            f"Array sample: {arr.flat[:min(10, arr.size)]}"
        )

    # Check for extremely large values that might cause numerical issues
    max_val = np.max(np.abs(arr))
    if max_val > 1e6:
        print(f"Warning: {name} contains very large values (max abs: {max_val:.2e})")


def validate_reward(
    reward: float,
    frame_number: int,
    details: dict = None
) -> None:
    """Validate reward value"""
    if reward is None:
        raise InvalidRewardError(f"Reward is None for frame {frame_number}")
    
    if not isinstance(reward, (int, float, np.number)):
        raise InvalidRewardError(
            f"Reward must be numeric, got {type(reward)} for frame {frame_number}"
        )
    
    if math.isnan(reward):
        raise InvalidRewardError(
            f"Reward is NaN for frame {frame_number}. Details: {details}"
        )
    
    if math.isinf(reward):
        raise InvalidRewardError(
            f"Reward is infinite ({reward}) for frame {frame_number}. Details: {details}"
        )
    
    # Check for extremely large rewards that might indicate calculation errors
    if abs(reward) > 1000:
        print(f"Warning: Very large reward ({reward:.2f}) for frame {frame_number}")

================
File: src/pyencoder/environment/train_refactor.py
================
import argparse
from pathlib import Path

from pyencoder.environment.naive_env import Av1GymEnv
from pyencoder.states.naive import NaiveState
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.monitor import Monitor


def prase_arg():
    parser = argparse.ArgumentParser(description="Train RL agent for video encoding")

    # Video and output
    parser.add_argument(
        "--file", help="Input video file", default="Data/bridge_close_qcif.y4m"
    )
    parser.add_argument(
        "--output_dir", default="logs/", help="Output directory for models and logs"
    )

    # RL parameters
    parser.add_argument(
        "--algorithm", choices=["ppo", "dqn"], default="ppo", help="RL algorithm to use"
    )
    parser.add_argument(
        "--total_iteration", type=int, default=50, help="Total training loop iterations, number of times the environment is reset"
    )
    parser.add_argument(
        "--learning_rate", type=float, default=3e-4, help="Learning rate"
    )
    parser.add_argument("--batch_size", type=int, default=64, help="Batch size")
    parser.add_argument(
        "--n_steps",
        type=int,
        default=-1,
        help="Number of steps per update (PPO only), should match number of frames in the video, -1 would search for the video length",
    )

    # Environment parameters
    parser.add_argument(
        "--lambda_rd", type=float, default=0.1, help="Rate-distortion lambda"
    )
    parser.add_argument(
        "--max_frames", type=int, default=100, help="Maximum frames per episode"
    )

    # Training parameters
    parser.add_argument(
        "--eval_freq", type=int, default=5000, help="Evaluation frequency"
    )
    parser.add_argument(
        "--save_freq", type=int, default=10000, help="Model save frequency"
    )

    parser.add_argument(
        "--disable_observation_normalization", 
        action="store_true", 
        help="Disable observation state normalization"
    )

    args = parser.parse_args()
    return args

    # Create trainer and run pipeline
    # trainer = VideoEncodingTrainer(args)
    # trainer.run_complete_pipeline()


if __name__ == "__main__":

    args = prase_arg()

    # create envirnment
    base_output_path = Path(args.output_dir)
    gyn_env = Av1GymEnv(
        video_path=args.file,
        output_dir=base_output_path,
        lambda_rd=args.lambda_rd,
        state=NaiveState,
    )
    
    env = Monitor(gyn_env, str(base_output_path / "monitor"))
    if args.n_steps == -1:
        # Automatically determine n_steps based on video length
        video_length = gyn_env.num_frames
        args.n_steps = video_length if video_length > 0 else 1000  # Fallback to 1000 if length is unknown

    model = None
    match args.algorithm.lower():
        case "ppo":
            model = PPO(
                "MlpPolicy",
                env,
                learning_rate=args.learning_rate,
                n_steps=args.n_steps,
                batch_size=args.batch_size,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,
                vf_coef=0.5,
                max_grad_norm=0.5,
                verbose=1,
            )
        case "dqn":
            model = DQN(
                "MlpPolicy",
                env,
                learning_rate=args.learning_rate,
                buffer_size=100000,
                learning_starts=1000,
                batch_size=args.batch_size,
                tau=1.0,
                gamma=0.99,
                train_freq=4,
                gradient_steps=1,
                target_update_interval=1000,
                exploration_fraction=0.1,
                exploration_initial_eps=1.0,
                exploration_final_eps=0.02,
                verbose=1,
            )
        case other:
            raise ValueError(f"Unsupported algorithm '{other}'. Choose either 'ppo' or 'dqn'.")
        
    total_timesteps = args.total_iteration * gyn_env.num_frames

    # training
    try:
        model.learn(
            total_timesteps=total_timesteps,
            # callback=callbacks,
            tb_log_name=f"{args.algorithm}_run",
        )

        # Save final model
        final_model_path = base_output_path / f"final_{args.algorithm}_model"
        model.save(str(final_model_path))
        gyn_env.save_bitstream_to_file(
            str(base_output_path / "final_encoder_video.ivf")
        )
        print(f"Training completed! Final model saved to: {final_model_path}")

    except KeyboardInterrupt:
        print("Training interrupted by user")
        # Save current model
        interrupted_model_path = (
            base_output_path / f"interrupted_{args.algorithm}_model"
        )
        model.save(str(interrupted_model_path))
        gyn_env.save_bitstream_to_file(
            str(base_output_path / "interrupted_encoder_video.ivf"),
            interrupt=True
        )
        print(f"Model saved to: {interrupted_model_path}")

================
File: src/pyencoder/states/abstract.py
================
from abc import ABC, abstractmethod
from typing import Any
from numpy import ndarray
from pyencoder import SuperBlockInfo
from pyencoder.utils.video_reader import VideoReader
import gymnasium as gym
from pyencoder.environment.av1_runner import Observation


class AbstractState(ABC):
    """
    Abstract base class for states in the PyEncoder framework.
    """
    
    @abstractmethod
    def __init__(self, video_reader: VideoReader, baseline_observations: list[Observation], sb_size: int = 64,
                 **kwargs: Any):
        """
        Initialize the state with any necessary parameters.
        
        Parameters:
            **kwargs: Additional keyword arguments for state initialization.
        """
        pass

    @abstractmethod
    def get_observation(self, 
                        frame: ndarray,
                        sbs: list[SuperBlockInfo], 
                        frame_type: int,
                        picture_number: int,
                        **kwargs) -> ndarray:
        """
        Get the current observation of the state.
        Parameters:
            frame ndarray: The current frame.
            SB_SIZE (int): Size of the state buffer, default is 64.
            **kwargs: Additional keyword arguments for processing the frame.
        
        Return a 1D numpy array with any size
        
        Note that need to handle inf or nan values if present.
        """
        pass
    
    @abstractmethod
    def get_observation_length(self) -> int:
        """
        Get the shape of the observation.
        
        Return an integer as the length of the 1D numpy array.
        """
        pass
    
    @abstractmethod
    def get_observation_space(self) -> gym.spaces.Space:
        """
        Get the observation space of the state.
        
        Return a gymnasium Space object representing the observation space.
        """
        pass

================
File: src/pyencoder/states/naive.py
================
from pyencoder import SuperBlockInfo
from .abstract import AbstractState
from numpy import ndarray
from typing import Any, Dict, List, Optional
from pyencoder.environment.av1_runner import Observation
import gymnasium as gym
from pyencoder.utils.video_reader import VideoReader
import numpy as np

# WARNING!! DO NOT CHANGE THE NAME OF THIS CLASS
class NaiveState(AbstractState):
    """
    A naive implementation of the State_templete class.
    This class provides a simple way to handle states without complex processing.
    """

    def __init__(self, video_reader: VideoReader, baseline_observations: list[Observation], sb_size: int = 64,
                 **kwargs: Any):
        """
        Initialize the NaiveState with flexible arguments.
        Only one of frame, (width and height), or num_sb should be provided.
        """
        self.sb_size = sb_size
        self.num_sb = video_reader.get_num_superblock()
        self.frame_count = video_reader.get_frame_count()
        array_length = self.get_observation_length()
        self.max_values = np.full(array_length, -np.inf, dtype=np.float32)
        for raw_obs in baseline_observations:
            frame = video_reader.read_frame(frame_number=raw_obs.picture_number)
            if frame is None or len(frame) == 0:
                continue
            obs = self.get_observation(frame, raw_obs.superblocks, raw_obs.frame_type, raw_obs.picture_number)
            self.max_values = np.maximum(self.max_values, obs)

    def get_observation(self,
                        frame: ndarray,
                        sbs: list[SuperBlockInfo], 
                        frame_type: int,
                        picture_number: int,
                        **kwargs) -> ndarray:
        """
        Get the current observation of the state. Promise to normalize the observation.
        
        Parameters:
            frame ndarray: The current frame.
            SB_SIZE (int): Size of the state buffer, default is 64.
            **kwargs: Additional keyword arguments for processing the frame.
        
        Returns:
            ndarray: A 1D numpy array with any size, handling inf or nan values if present.
        """
        h, w = frame.shape[:2]
        y_comp_list = []
        h_mv_list = []
        v_mv_list = []
        beta_list = []

        for y in range(0, h, self.sb_size):
            for x in range(0, w, self.sb_size):  # follow encoder order, x changes first
                y_end = min(y + self.sb_size, h)
                x_end = min(x + self.sb_size, w)
                sb = frame[y:y_end, x:x_end]
                if sb.size == 0:
                    continue

                sb_y_var = np.var(sb[:, :, 0])  # Y-component variance
                sb_x_mv = np.mean(sb[:, :, 1])  # Horizontal motion vector
                sb_y_mv = np.mean(sb[:, :, 2])  # Vertical motion vector
                beta = np.mean(np.abs(sb))  # Example metric

                y_comp_list.append(sb_y_var)
                h_mv_list.append(sb_x_mv)
                v_mv_list.append(sb_y_mv)
                beta_list.append(beta)

        obs = np.array([y_comp_list, h_mv_list, v_mv_list, beta_list], dtype=np.float32).flatten()
        # check for inf or nan values and handle them
        # if illegal, replace with self.max_values at the corresponding index
        obs = np.where(np.isfinite(obs), obs, self.max_values)
        return obs

    def get_observation_length(self) -> int:
        """
        Get the shape of the observation.
        
        Return an integer as the length of the 1D numpy array.
        """
        return 4 * self.num_sb
    
    def get_observation_space(self) -> gym.spaces.Space:
        """
        Get the observation space of the state.
        
        Returns:
            gym.spaces.Space: A gymnasium Space object representing the observation space.
        """
        return gym.spaces.Box(
            low=-float('inf'),
            high=float('inf'),
            shape=(self.get_observation_length(),),
            dtype=np.float32
        )

================
File: src/pyencoder/utils/video_reader.py
================
import csv
import enum
from typing import Literal, Optional, Tuple, TypeAlias, cast

import cv2
import numpy as np
from numpy import ndarray

from typing import Optional, Tuple
import os

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.colors import LinearSegmentedColormap
from pyencoder.environment.constants import SB_SIZE
import seaborn as sns

class VideoComponent(enum.Enum):
    Y = "Y"
    Cb = "Cb"
    Cr = "Cr"

class VideoReader:
    def __init__(self, path: str):
        self.path = path
        self.cap = cv2.VideoCapture(path)
        if not self.cap.isOpened():
            raise ValueError(f"Cannot open video file: {path}")
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    def read_frame(self, frame_number) -> Optional[np.ndarray]: # (H, W, 3)
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
        ret, frame = self.cap.read()
        return frame if ret else None

    def release(self):
        self.cap.release()

    def get_resolution(self) -> Tuple[int, int]:
        return self.width, self.height

    def read_ycrcb_components(self, frame_number: int) -> Optional[np.ndarray]: # (H, W, 3)
        rgb_frame = self.read_frame(frame_number=frame_number)
        if rgb_frame is None:
            return None
        ycrcb_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2YCrCb)
        return ycrcb_frame

    def get_frame_count(self) -> int:
        return int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))

    def render_frame_number(self, frame_number: int):
        frame = self.read_frame(frame_number=frame_number)
        if frame is not None:
            self.render_frame(frame)

    def render_frame(self, frame: np.ndarray):
        cv2.imshow("Frame", frame)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    # sb info
    def get_num_superblock(self):
        num_blocks_h = (self.height + SB_SIZE - 1) // SB_SIZE
        num_blocks_w = (self.width + SB_SIZE - 1) // SB_SIZE
        return num_blocks_h * num_blocks_w

    def ycrcb_psnr(
        self,
        frame_number: int,
        other_frame: np.ndarray, # (H, W, 3)
        baseline_heighest_psnr
    ):
        """
        frame number
        other frame: (y,cb,cr)
        should be same size
        """
        target_components = self.read_ycrcb_components(frame_number)
        if target_components is None:
            raise ValueError(f"Unable to read frame {frame_number} from the video.")

        if target_components.shape != other_frame.shape:
            raise ValueError("Dimension mismatch between video frame and reference frame components.")

        y_psnr = VideoReader.compute_psnr(target_components[:, :, 0], other_frame[:, :, 0], baseline_heighest_psnr["y"])
        cb_psnr = VideoReader.compute_psnr(target_components[:, :, 1], other_frame[:, :, 1], baseline_heighest_psnr['cb'])
        cr_psnr = VideoReader.compute_psnr(target_components[:, :, 2], other_frame[:, :, 2], baseline_heighest_psnr['cr'])

        # render the image for debug 
        # target_bgr = cv2.cvtColor(target_components, cv2.COLOR_YCrCb2BGR)
        # other_bgr = cv2.cvtColor(other_frame, cv2.COLOR_YCrCb2BGR)
        # cv2.imwrite(f"target_{frame_number}.png", target_bgr)
        # cv2.imwrite(f"other_frame_{frame_number}.png", other_bgr)
        return y_psnr, cb_psnr, cr_psnr

    @staticmethod
    def render_single_component(
        component_array: np.ndarray, component_type: VideoComponent
    ):
        cv2.imshow(str(component_type.value), component_array)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    @staticmethod
    def render_components(y: np.ndarray, cb: np.ndarray, cr: np.ndarray):
        # OpenCV uses Y, Cr, Cb order
        ycrcb_image = cv2.merge((y, cr, cb))

        bgr_image = cv2.cvtColor(ycrcb_image, cv2.COLOR_YCrCb2BGR)
        cv2.imshow("BGR", bgr_image)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    @staticmethod
    def compute_psnr(target: np.ndarray, reference: np.ndarray, baseline_heighest_psnr: float = 100.0):
        return min(cv2.PSNR(target, reference), baseline_heighest_psnr)


# # simple test
# if __name__ == "__main__":
#     reader = VideoReader("/home/tom/tmp/playground/akiyo_qcif.y4m")

#     reader.get_resolution()
#     reader.get_frame_count()
#     y, cb, cr = reader.read_ycrcb_components(1)

#     # Flatten arrays and write to CSV
#     with open("frame1_ycrcb.csv", "w", newline="") as csvfile:
#         writer = csv.writer(csvfile)
#         writer.writerow(["Component", "Row", "Col", "Value"])
#         for comp_name, comp_array in zip(["Y", "Cb", "Cr"], [y, cb, cr]):
#             for row in range(comp_array.shape[0]):
#                 for col in range(comp_array.shape[1]):
#                     writer.writerow([comp_name, row, col, int(comp_array[row, col])])

#     # reader.render_single_component(y, VideoComponent.Y)

#     VideoReader.render_components(y, cb, cr)

================
File: src/pyencoder/__init__.py
================
from dataclasses import dataclass
from importlib import import_module as _imp
from typing import TypedDict

av1_wrapper = _imp(".av1_wrapper", package=__name__)
_run = av1_wrapper.run
_register = av1_wrapper.register_callbacks

class SuperBlockInfo(TypedDict):
    sb_org_x: int
    sb_org_y: int
    sb_width: int
    sb_height: int
    sb_qindex: int
    sb_x_mv: int
    sb_y_mv: int

def run(**kwargs):
    argv = ["svtav1"]
    for key, val in kwargs.items():
        # If key starts with '-', use it directly as a flag
        if key.startswith("-"):
            flag = key
        else:
            # Determine short or long flag
            flag = f"-{key}" if len(key) == 1 else f"--{key.replace('_', '-')}"

        # Convert value to string appropriately
        if isinstance(val, bool):
            argv.extend([flag, "1" if val else "0"])
        else:
            argv.extend([flag, str(val)])

    _run(argv)


def register_callbacks(*, get_deltaq_offset=None, picture_feedback=None):
    _register(get_deltaq_offset, picture_feedback)

================
File: src/pyencoder/_binding.cpp
================
extern "C" {
    #define main app_main
    #include "../Source/App/app_main.c"
    #undef main
}

#ifndef SVT_ENABLE_USER_CALLBACKS
#define SVT_ENABLE_USER_CALLBACKS 1
#endif

#include <pybind11/pybind11.h>

#include "../bridge/utils.hpp"
#include "../bridge/cb_registration.hpp"
#include "../bridge/pybridge.h"

#include "../Source/Lib/Globals/enc_callbacks.h"
#include "../Source/API/EbSvtAv1Enc.h"

#include <vector>
#include <string>

namespace py = pybind11;
using namespace pybridge;

int init_callbacks();
void deinit_callbacks();

// run(argv: List[str]) -> None
static py::object run(py::list py_argv)
{
    const int argc = static_cast<int>(py_argv.size());

    // Keep string storage alive for the whole call.
    std::vector<std::string> storage;
    storage.reserve(argc);

    std::vector<char *> argv;
    argv.reserve(argc + 1);

    // Parse args to argv and argc list
    for (const py::handle &item : py_argv) {
        storage.emplace_back(py::cast<std::string>(item));
        argv.push_back(const_cast<char *>(storage.back().c_str()));
    }
    argv.push_back(nullptr);

    int rc = 0;
    {
        // Release the GIL while the encoder CLI runs.
        py::gil_scoped_release release;
        rc = app_main(argc, argv.data());
        py::gil_scoped_acquire acquire;
        deinit_callbacks();
    }

    if (rc != 0) {
        throw std::runtime_error(
            "SvtAv1EncApp returned non‑zero exit code " + std::to_string(rc));
    }

    return py::none();
}

// register_callbacks(get_deltaq_offset=None, frame_feedback=None, picture_feedback=None) -> None
static py::object register_callbacks(py::object py_get_deltaq_offset = py::none(),
                                     py::object py_picture_feedback  = py::none())
{
    init_callbacks();

    // Store the user callables and hook up the C trampolines
    pybridge_set_cb(CallbackEnum::GetDeltaQOffset, py_get_deltaq_offset);
    pybridge_set_cb(CallbackEnum::RecvPictureFeedback, py_picture_feedback);

    // Tell SVT‑AV1 about the trampolines
    static PluginCallbacks cbs;
    cbs.user_get_deltaq_offset = get_deltaq_offset_cb;
    cbs.user_picture_feedback = recv_picture_feedback_cb;

    if (svt_av1_enc_set_callbacks(&cbs) != EB_ErrorNone) {
        throw std::runtime_error("failed to set callbacks");
    }

    return py::none();
}

PYBIND11_MODULE(av1_wrapper, m)
{
    m.doc() = "In‑process bindings for the SVT‑AV1 encoder CLI";

    m.def("run", &run,
          "Run the SVT‑AV1 encoder CLI in‑process.");

    m.def("register_callbacks", &register_callbacks,
          py::arg("get_deltaq_offset") = py::none(),
          py::arg("picture_feedback")  = py::none(),
          "Attach callbacks to the SVT‑AV1 encoder.");
}

int init_callbacks()
{
    for (int i = 0; i < static_cast<int>(CallbackEnum::Count); ++i) {
        g_callbacks[i] = new Callback{py::none(), nullptr, 0};
    }
    g_callbacks[0]->n_args = 3; // GetDeltaQOffset
    g_callbacks[1]->n_args = 3; // RecvPictureFeedback
    return 0;
}

void deinit_callbacks()
{
    for (int i = 0; i < static_cast<int>(CallbackEnum::Count); ++i) {
        delete g_callbacks[i];
        g_callbacks[i] = nullptr;
    }
}

================
File: CMakeLists.txt
================
# Header‑only but still adds a target

if(${FROM_PIP_INSTALL}) 
    message(STATUS "Building for pip install")
    cmake_minimum_required(VERSION 3.20)
    project(av1_gym_bridge LANGUAGES C CXX ASM)

    # Global compile options
    set(CMAKE_C_STANDARD   11)
    set(CMAKE_CXX_STANDARD 17)
    set(CMAKE_CXX_STANDARD_REQUIRED ON)
    set(CMAKE_POSITION_INDEPENDENT_CODE ON)
    set(CMAKE_C_FLAGS "-O0 ${CMAKE_C_FLAGS}")
    set(CMAKE_CXX_FLAGS "-O0 ${CMAKE_CXX_FLAGS}")

    add_compile_definitions(SVT_ENABLE_USER_CALLBACKS)

    # Compile in the upstream SVT‑AV1 LIB
    set(BUILD_SHARED_LIBS OFF  CACHE BOOL "" FORCE)
    set(BUILD_APPS        OFF  CACHE BOOL "" FORCE)

    add_subdirectory(
        ${CMAKE_CURRENT_LIST_DIR}/..
        ${CMAKE_CURRENT_BINARY_DIR}/svtcore
        EXCLUDE_FROM_ALL)
endif()

find_package(Threads  REQUIRED)
find_package(Python3  REQUIRED COMPONENTS Interpreter Development)
find_package(pybind11 CONFIG REQUIRED) 

# Add Safe‑string library
file(GLOB SAFE_SRC "${CMAKE_CURRENT_LIST_DIR}/../third_party/safestringlib/*.c")
add_library(safeclib STATIC ${SAFE_SRC})
target_include_directories(safeclib PUBLIC
    "${CMAKE_CURRENT_LIST_DIR}/../third_party/safestringlib")

# Compole plugin callbacks
add_library(svtav1_plugin STATIC
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals/enc_callbacks.c")

target_include_directories(svtav1_plugin PRIVATE
    "${CMAKE_CURRENT_LIST_DIR}/.."
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API")

target_link_libraries(svtav1_plugin PRIVATE Threads::Threads)

# Re‑build Source/App/ as a static library, renaming main()
file(GLOB APP_SRC "${CMAKE_CURRENT_LIST_DIR}/../Source/App/*.c")
add_library(svtav1_app STATIC ${APP_SRC})

target_compile_definitions(svtav1_app PRIVATE main=svt_enc_app_main)
target_include_directories(svtav1_app PUBLIC
    "${CMAKE_CURRENT_LIST_DIR}/.."
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/App"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API")

target_link_libraries(svtav1_app PUBLIC Threads::Threads safeclib)

# Build the Python extension  av1_wrapper.{so,dylib,pyd}
set(BRIDGE_SRC
    src/bridge/pybridge.cpp
    src/bridge/cb_registration.cpp
    src/bridge/utils.cpp)

set(PYENCODER_SRC
    src/pyencoder/_binding.cpp
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals/rl_feedback.c")

add_library(av1_wrapper MODULE
    ${BRIDGE_SRC}
    ${PYENCODER_SRC})

# On Windows a Python extension must be named *.pyd without the "lib" prefix.
if (WIN32)
    set_target_properties(av1_wrapper PROPERTIES
        OUTPUT_NAME "av1_wrapper"
        PREFIX      ""
        SUFFIX      ".pyd")
else()
    set_target_properties(av1_wrapper PROPERTIES PREFIX "")
endif()

# Include paths
target_include_directories(av1_wrapper PRIVATE
    "${CMAKE_CURRENT_LIST_DIR}/.."
    src/bridge
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/App"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals"
    ${Python3_INCLUDE_DIRS})

# Link targets
target_link_libraries(av1_wrapper
    PRIVATE
        pybind11::module
        svtav1_plugin
        svtav1_app
        SvtAv1Enc
        safeclib
        Python3::Python
        Threads::Threads
        m)                   # libm on *nix

# Installation path for scikit‑build‑core
if(${FROM_PIP_INSTALL})
    install(TARGETS av1_wrapper
        LIBRARY DESTINATION .
        RUNTIME DESTINATION .)
else()
    install(TARGETS av1_wrapper
        LIBRARY DESTINATION ${Python3_SITEARCH}/pyencoder
        RUNTIME DESTINATION ${Python3_SITEARCH}/pyencoder)
endif()

================
File: monitor.monitor.csv
================
#{"t_start": 1751249221.963371, "env_id": "None"}
r,l,t
0.0,300,8.070627
0.0,300,15.690531
0.0,300,23.601382

================
File: pyproject.toml
================
[project]
name = "av1-gym"
dynamic = ["version"]
authors = [{ name = "TL26" }]
readme = "README.md"
requires-python = ">=3.9"
dependencies = ["gymnasium>=0.29", "numpy>=2.1.3", "opencv-python>= 4.10", "pybind11>=2.13.6"]

[build-system]
requires = [
  "scikit-build-core>=0.11",
  "setuptools_scm>=7",
  "pybind11_stubgen>=2"
]
build-backend = "scikit_build_core.build"

[tool.scikit-build]
wheel.packages    = ["src/pyencoder"]
wheel.install-dir = "pyencoder"
install.strip = false

[tool.scikit-build.cmake]
define = { SVT_ENABLE_USER_CALLBACKS = "ON" }
args = [
    "-DCMAKE_BUILD_TYPE=Debug",
    "-DCMAKE_C_FLAGS=-g",
    "-DCMAKE_CXX_FLAGS=-g",
    "-DCMAKE_INSTALL_DO_STRIP=OFF",
    "-DFROM_PIP_INSTALL=1",
]

================
File: test.py
================
import pyencoder

def get_deltaq_offset(sbs: list[pyencoder.SuperBlockInfo], frame_type: int, frame_number: int) -> list[int]:
    print("In python: ", len(sbs), frame_type, frame_number)
    return [0]*len(sbs)

def picture_feedback(a, b, c):
    pass

pyencoder.register_callbacks(get_deltaq_offset=get_deltaq_offset, picture_feedback=picture_feedback)

args = {
    "input": "../../playground/bus_cif.y4m",
    "pred_struct": 1,
    "rc": 2,
    "tbr": 100,
    "enable_stat_report": True
}

pyencoder.run(**args)



================================================================
End of Codebase
================================================================
