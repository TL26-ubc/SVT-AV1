This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
src/
  av1gym/
    environment/
      utils/
        video_reader.py
      __init__.py
      constants.py
      environment.py
      runner.py
    pyencoder/
      bridge/
        cb_registration.cpp
        cb_registration.hpp
        pybridge.cpp
        pybridge.h
        utils.cpp
        utils.hpp
      __init__.py
      _binding.cpp
      _binding.py
    __init__.py
CMakeLists.txt
pyproject.toml
test.py

================================================================
Files
================================================================

================
File: src/av1gym/environment/utils/video_reader.py
================
import enum
import av
import cv2
import numpy as np
from av1gym.environment.constants import SB_SIZE

class VideoComponent(enum.Enum):
    Y = "Y"
    Cb = "Cb"
    Cr = "Cr"

class VideoReader:
    def __init__(self, path: str):
        self.path = path
        self.container = av.open(path)
        self.video_stream = self.container.streams.video[0]
        if self.video_stream is None:
            raise ValueError(f"Cannot open video file: {path}")
        self.width = self.video_stream.width
        self.height = self.video_stream.height
        self._frame_count = None
        self._frames_cache = {}  # Cache frames for random access

    def _ensure_frame_count(self) -> int:
        """Ensure frame count is calculated"""
        if self._frame_count is None:
            if self.video_stream.frames > 0:
                self._frame_count = self.video_stream.frames
            else:
                # Manual count if metadata unreliable
                count = 0
                for _ in self.container.decode(video=0):
                    count += 1
                self._frame_count = count
                self.container.seek(0)
        return self._frame_count

    def _get_frame_at_index(self, frame_number) -> av.VideoFrame | None:
        """Get PyAV frame at specific index"""
        if frame_number in self._frames_cache:
            return self._frames_cache[frame_number]
        
        # Reset container and iterate to target frame
        self.container.seek(0)
        current_index = 0
        for frame in self.container.decode(video=0):
            if current_index == frame_number:
                self._frames_cache[frame_number] = frame
                return frame
            current_index += 1
        return None

    def read_frame_raw(self, frame_number) -> np.ndarray: 
        # The current frame in yuv420p format, shape (3/2 * H, W).
        av_frame = self._get_frame_at_index(frame_number)
        if av_frame is None:
            raise RuntimeError('This frame should exist')
        
        # yuv420p format conversion
        return av_frame.to_ndarray(format='yuv420p')

    def release(self):
        self.container.close()
        self._frames_cache.clear()

    def get_resolution(self) -> tuple[int, int]:
        return self.width, self.height

    def read_frame(self, frame_number: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]: # (w, h), (w // 2, h // 2), (w // 2, h // 2)
        ycrcb_frame = self.read_frame_raw(frame_number)
        return self.get_yuv_planes(ycrcb_frame)

    def get_frame_count(self) -> int:
        return self._ensure_frame_count()

    # sb info
    def get_superblock_dims(self) -> tuple[int, int]:
        num_blocks_h = (self.height + SB_SIZE - 1) // SB_SIZE
        num_blocks_w = (self.width + SB_SIZE - 1) // SB_SIZE
        return num_blocks_w, num_blocks_h

    @staticmethod
    def compute_psnr(target: np.ndarray, reference: np.ndarray) -> float:
        return cv2.PSNR(target, reference)

    @staticmethod
    def compute_mse(target: np.ndarray, reference: np.ndarray) -> float:
        return np.mean((target.astype(np.float32) - reference.astype(np.float32)) ** 2).astype(float)
    
    def get_yuv_planes(self, frame: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        w, h = self.width, self.height
        y_plane = frame[0 : w*h].reshape((h, w))
        u_plane = frame[w*h : w*h + (w//2)*(h//2)].reshape((h//2, w//2))
        v_plane = frame[w*h + (w//2)*(h//2) : ].reshape((h//2, w//2))
        return y_plane, u_plane, v_plane

================
File: src/av1gym/environment/__init__.py
================
from gymnasium.envs.registration import register
from .environment import Av1GymEnv

register(
    id="Av1Env-v0",
    entry_point="av1gym.environment:Av1GymEnv",
)

__all__ = ["Av1GymEnv"]

================
File: src/av1gym/environment/constants.py
================
QP_MIN, QP_MAX = -3, 3  # delta QP range which will be action
SB_SIZE = 64 # superblock size
BIT_DEPTH = 255

================
File: src/av1gym/environment/environment.py
================
import threading
from pathlib import Path
import gymnasium as gym
import numpy as np
from .runner import Av1Runner, Feedback
from .utils.video_reader import VideoReader
from .constants import QP_MIN, QP_MAX
import math

SB_FEATURES = 4
FRAME_FEATURES = 1

# Extending gymnasium's Env class
# https://gymnasium.farama.org/api/env/#gymnasium.Env
class Av1GymEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(
        self,
        video_path: str,
        output_dir: str,
        *,
        lambda_rd: float = 0.1,
    ):
        super().__init__()
        self.video_path = Path(video_path)
        self.output_dir = Path(output_dir)
        self.av1_runner = Av1Runner(video_path)

        self.lambda_rd = lambda_rd
        self._episode_done = threading.Event()

        # Initialize the VideoReader
        self.video_reader = VideoReader(path=video_path)

        self.sb_w, self.sb_h = self.video_reader.get_superblock_dims()
        self.num_sb = self.sb_w * self.sb_h
        self.num_frames = self.video_reader.get_frame_count()

        # Action space, QP offset grid
        self.action_space = gym.spaces.MultiDiscrete(
            # num \in [QP_MAX, QP_MIN], there are num_superblocks of them
            nvec=np.full(self.num_sb, QP_MAX - QP_MIN + 1, dtype=np.int32)
        )

        # Observation space, global and per sb observations
        self.observation_space = gym.spaces.Dict({
            "superblock": gym.spaces.Box(-np.inf, np.inf, (self.sb_w, self.sb_h, SB_FEATURES), np.float32),
            "frame": gym.spaces.Box(-np.inf, np.inf, (FRAME_FEATURES,), np.float32),
        })

        # Episode management
        self.current_frame = 0
        self.terminated = False
        
    # https://gymnasium.farama.org/api/env/#gymnasium.Env.reset
    def reset(
        self, 
        *, 
        seed: int | None = None, 
        options: dict | None = None
    ) -> tuple[dict, dict]:
        print("Resetting environment...")

        super().reset(seed=seed)

        # Reset episode state
        self.current_frame = 0
        self.current_episode_reward = 0.0
        self.terminated = False

        # Start encoder in separate thread
        self.av1_runner.run()

        # Get initial observation
        initial_obs = self._get_next_observation()

        info = {
            "frame_number": self.current_frame,
            "reward": 0,
            "bitstream_size": 0,
            "episode_frames": self.current_frame,
        }

        return initial_obs, info

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.step
    def step(
        self,
        action: np.ndarray
    ) -> tuple[dict, float, bool, bool, dict]:
        if self.terminated:
            raise RuntimeError("Episode has ended. Call reset() before step().")

        # Validate action
        if action.shape != (self.num_sb,):
            raise ValueError(
                f"Action shape {action.shape} != expected ({self.num_sb},)"
            )
          
        # Validate action values
        validate_array(action, f"Action for frame {self.current_frame}")
        
        # Convert action to QP offsets
        qp_offsets = self._action_to_qp_offsets(action)
        
        # Send action response to encoder
        self.av1_runner.send_action_response(action=qp_offsets.tolist())
        
        # Wait for encoding feedback
        feedback = self.av1_runner.wait_for_feedback()

        # Calculate reward
        reward = self._calculate_reward(feedback, qp_offsets)
        self.current_episode_reward += reward

        # Update episode state
        self.current_frame += 1

        # Check termination conditions
        self._check_termination_conditions()
        
        # Get next observation with validation
        if self.terminated:
            # Episode has ended, return dummy observation
            next_obs = dict()
            print(f"Episode terminated at frame {self.current_frame-1} (total frames: {self.num_frames})")
        else:
            # Get next observation with validation
            next_obs = self._get_next_observation()
        
        info = {
            "frame_number": self.current_frame,
            "reward": reward,
            "bitstream_size": feedback.bitstream_size,
            "episode_frames": self.current_frame,
        }

        return next_obs, reward, self.terminated, False, info

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.close
    def close(self):
        print("Closing environment...")

        if self.av1_runner:
            self.av1_runner.join()
        
        print("Environment closed.")

    # https://gymnasium.farama.org/api/env/#gymnasium.Env.render
    def render(self):
        pass
    
    def save_bitstream_to_file(self, output_path: str, interrupt: bool = False):
        """Save the bitstream to a file"""
        self.av1_runner.save_bitstream_to_file(output_path, interrupt=interrupt)

    def _get_next_observation(self) -> dict:
        """Get current observation based on current frame"""
        try:
            observation = self.av1_runner.wait_for_next_observation()
            y_plane, _, _ = self.video_reader.read_frame(frame_number=observation.picture_number)

            # Assert frame numbers match
            assert (
                observation.picture_number == self.current_frame
            ), f"observation frame {observation.picture_number} != current_frame {self.current_frame}"

            # Build observations for each sb
            superblock_obs = []
            for superblock in observation.superblocks:
                sb_w, sb_h = superblock["sb_width"], superblock["sb_height"]
                sb_org_x, sb_org_y = superblock["sb_org_x"], superblock["sb_org_y"]
                sb_y_values = y_plane[sb_org_y:sb_org_y+sb_h, sb_org_x:sb_org_x+sb_w]

                # Build obs
                superblock_obs.append([
                    superblock["sb_qindex"],
                    superblock["sb_x_mv"],
                    superblock["sb_y_mv"],
                    sb_y_values.var()
                ])
            
            return dict(
                superblock=superblock_obs,
                frame=[observation.frame_type],
            )
            
        except Exception as e:
            raise RuntimeError(f"Failed to get current observation for frame {self.current_frame}: {e}")

    def _action_to_qp_offsets(self, action: np.ndarray) -> np.ndarray:
        """Convert discrete action to QP offsets"""
        # Map from [0, QP_MAX-QP_MIN] to [QP_MIN, QP_MAX]
        qp_offsets = action + QP_MIN
        qp_offsets = np.clip(qp_offsets, QP_MIN, QP_MAX)
        
        # Validate QP offsets
        validate_array(qp_offsets, f"QP offsets for frame {self.current_frame}")
        
        return qp_offsets

    def _calculate_reward(
        self, feedback: Feedback, qp_offsets: np.ndarray
    ) -> float:
        """Calculate reward based on encoding feedback"""
        try:
            # Get postencoded frame data
            postencoded_frame = feedback.encoded_frame_data

            # Convert to yuv planes
            postencoded_y_plane, _, _ = self.video_reader.get_yuv_planes(postencoded_frame)
            original_y_plane, _, _ = self.video_reader.read_frame(frame_number=feedback.picture_number)
            
            # Calculate mse as reward
            mse = VideoReader.compute_mse(postencoded_y_plane, original_y_plane)
            
            return mse
        except Exception as e:
            raise RuntimeError(f"Failed to calculate reward for frame {self.current_frame}: {e}")

    def _check_termination_conditions(self):
        """Check if episode should terminate"""
        # Maximum frames reached
        if self.current_frame >= self.num_frames:
            self.terminated = True


def validate_array(arr: np.ndarray, name: str) -> None:
    """Validate numpy array for NaN, inf, and other invalid values"""
    if arr is None:
        raise ValueError(f"{name} is None")

    if not isinstance(arr, np.ndarray):
        raise TypeError(f"{name} must be numpy array, got {type(arr)}")

    if arr.size == 0:
        raise ValueError(f"{name} is empty")

    # Check for NaN values
    nan_mask = np.isnan(arr)
    if np.any(nan_mask):
        nan_count = np.sum(nan_mask)
        nan_indices = np.where(nan_mask)
        raise RuntimeError(
            (
                (
                    f"{name} contains {nan_count} NaN values at indices: {nan_indices}. "
                    f"Array shape: {arr.shape}, dtype: {arr.dtype}\n"
                    f"Array sample: {arr.flat[:min(10, arr.size)]}"
                )
            )
        )

    # Check for infinite values
    inf_mask = np.isinf(arr)
    if np.any(inf_mask):
        inf_count = np.sum(inf_mask)
        inf_indices = np.where(inf_mask)
        raise RuntimeError(
            f"{name} contains {inf_count} infinite values at indices: {inf_indices}. "
            f"Array shape: {arr.shape}, dtype: {arr.dtype}\n"
            f"Array sample: {arr.flat[:min(10, arr.size)]}"
        )

    # Check for extremely large values that might cause numerical issues
    max_val = np.max(np.abs(arr))
    if max_val > 1e6:
        print(f"Warning: {name} contains very large values (max abs: {max_val:.2e})")

def validate_reward(
    reward: float,
    frame_number: int,
    details: dict
) -> None:
    """Validate reward value"""
    if reward is None:
        raise RuntimeError(f"Reward is None for frame {frame_number}")
    
    if not isinstance(reward, (int, float, np.number)):
        raise RuntimeError(
            f"Reward must be numeric, got {type(reward)} for frame {frame_number}"
        )
    
    if math.isnan(reward):
        raise RuntimeError(
            f"Reward is NaN for frame {frame_number}. Details: {details}"
        )
    
    if math.isinf(reward):
        raise RuntimeError(
            f"Reward is infinite ({reward}) for frame {frame_number}. Details: {details}"
        )
    
    # Check for extremely large rewards that might indicate calculation errors
    if abs(reward) > 1000:
        print(f"Warning: Very large reward ({reward:.2f}) for frame {frame_number}")

================
File: src/av1gym/environment/runner.py
================
import io
from queue import Queue
from pathlib import Path
from av1gym.pyencoder import SuperBlockInfo
import threading
from dataclasses import dataclass

import av
import cv2
import numpy as np
import av1gym.pyencoder as encoder

@dataclass
class Observation:
    superblocks: list[SuperBlockInfo]
    frame_type: int
    picture_number: int

@dataclass
class Action:
    skip: bool
    offsets: list[int] | None

@dataclass
class Feedback:
    """
    encoded_frame_data: np.ndarray, shape (3/2 * H, W)
    """
    picture_number: int
    bitstream_size: int
    encoded_frame_data: np.ndarray

global the_only_object
the_only_object = None

def picture_feedback_trampoline(bitstream: bytes, size: int, picture_number: int):
    assert the_only_object is not None
    the_only_object.picture_feedback(bitstream, size, picture_number)

def get_deltaq_offset_trampoline(sbs: list[SuperBlockInfo], frame_type: int, picture_number: int) -> list[int]:
    assert the_only_object is not None
    return the_only_object.get_deltaq_offset(sbs, frame_type, picture_number)

class Av1Runner:
    """
    A class to handle callbacks from Python to C for encoding video frames.
    This class is designed to be used with a C encoder that requires specific
    callbacks for frame processing.
    """

    def __init__(self, video_path):
        global the_only_object
        if the_only_object is not None:
            raise RuntimeError(
                "Av1Runner instance already exists. Only one instance is allowed."
            )
        the_only_object = self

        self.video_path = video_path
        self.bytes_keeper = {}
        self.previous_training_bytes_keeper = {}  # Store previous training bytes
        self.all_bitstreams = io.BytesIO()  # Holds joined bitstream data

        w, h = frame_dims_from_file(video_path)
        self.sb_total_count = superblocks_from_dims(w, h)

        self.first_round = True  # Flag for the first round of encoding
        self.first_round_byte_usage = {}  # Store byte usage for the first round

        # Synchronization
        self.observation_queue: Queue[Observation] = Queue(maxsize=1) # Encoder provides observation
        self.action_queue: Queue[Action] = Queue(maxsize=1) # RL provides action
        self.feedback_queue: Queue = Queue() # Encoder provides feedback to RL
        self.encoder_thread: threading.Thread | None = None # Encoding thread

    def run(self, output_path: str | None = None, block: bool = False):
        """
        Start the encoder in a new thread.
        If block is True, wait for the encoder to finish.
        """
        if self.encoder_thread is not None and self.encoder_thread.is_alive():
            print("Waiting for previous encoder thread to terminate...")
            self.encoder_thread.join(timeout=20.0)

        self.encoder_thread = threading.Thread(
            target=self._run_encoder,
            args=(output_path,),
            daemon=True,
            name="EncoderThread"
        )
        self.encoder_thread.start()

        if block:
            self.encoder_thread.join()

    def _run_encoder(self, output_path: str | None = None):
        print("Starting encoder thread...")
        self.reset()
        self.register_callbacks()

        args = {
            "input": self.video_path,
            "pred_struct": 1,
            "rc": 2,
            "tbr": 100,
            "enable_stat_report": True,
        }

        if output_path:
            args["b"] = output_path

        encoder.run(**args)

    def join(self):
        if not self.encoder_thread or not self.encoder_thread.is_alive():
            return
        else:
            self.encoder_thread.join()

    def register_callbacks(self):
        encoder.register_callbacks(
            get_deltaq_offset=get_deltaq_offset_trampoline,
            picture_feedback=picture_feedback_trampoline,
        )

    def reset(self):
        """
        Reset the callback state, clearing the bytes_keeper and all_bitstreams.
        This is typically called at the start of a new encoding session.
        """
        self.previous_training_bytes_keeper = self.bytes_keeper.copy()
        self.bytes_keeper.clear()
        self.all_bitstreams.close()
        self.all_bitstreams = io.BytesIO()

        # Clear queues
        while not self.observation_queue.empty():
            self.observation_queue.get_nowait()

        while not self.action_queue.empty():
            self.action_queue.get_nowait()

        while not self.feedback_queue.empty():
            self.feedback_queue.get_nowait()

    def join_bitstreams(self):
        joined_bitstream_num = 0
        while joined_bitstream_num in self.bytes_keeper.keys():
            self.all_bitstreams += self.bytes_keeper[joined_bitstream_num]
            joined_bitstream_num += 1

    def picture_feedback(self, bitstream: bytes, size: int, picture_number: int):
        """
        Callback for receiving encoded picture data from the C encoder.
        This sends feedback to the RL environment.
        """
        self.bytes_keeper[picture_number] = bitstream
        encoded_frame_data = self.get_last_frame(bitstream=bitstream)

        # Prepare feedback for RL environment
        feedback_data = Feedback(
            picture_number = picture_number,
            bitstream_size = size,
            encoded_frame_data = encoded_frame_data,
        )

        # Send feedback to RL environment (non-blocking)
        self.feedback_queue.put_nowait(feedback_data)

    def get_last_frame(self, bitstream):
        byte_file = self.all_bitstreams
        byte_file.write(bitstream)
        byte_file.seek(0)
        container = av.open(byte_file, 'r')
        last_frame = None
        for frame in container.decode(video=0):
            last_frame = frame

        assert last_frame != None
        ycrcb_array = last_frame.to_ndarray(format="yuv420p") # (3/2 * H, W)

        # if the last frame is a keyframe, we can write the bitstream to the file
        if last_frame.key_frame:
            byte_file.close()
            self.all_bitstreams = io.BytesIO()  # get a new bytefile
            self.all_bitstreams.write(bitstream)  # write the keyframe to bytefile
        container.close()
        return ycrcb_array

    def get_deltaq_offset(self, sbs: list[SuperBlockInfo], frame_type: int, picture_number: int) -> list[int]:
        """
        Callback to get QP offsets for superblocks in a frame.
        This method MUST return immediately as the encoder waits synchronously.
        """
        # Request action from RL environment
        observation = Observation(
            picture_number=picture_number,
            superblocks=sbs,
            frame_type=frame_type,
        )

        # Send action request to RL environment (blocking call)
        self.observation_queue.put_nowait(observation)

        # Wait for RL response
        action = self.action_queue.get()

        # Dummy offsets to skip
        if action.skip:
            return [114514] * self.sb_total_count

        if action.offsets is None:
            raise ValueError(f"Action response is null")

        if len(action.offsets) != self.sb_total_count:
            raise ValueError(f"Action response length mismatch. Expected {self.sb_total_count}, got {len(action.offsets)}")

        return action.offsets

    def wait_for_next_observation(self) -> Observation:
        """
        Wait for action request from encoder.
        Called by RL environment to get the next frame to process.
        """
        return self.observation_queue.get()

    def send_action_response(self, *, action: list[int] | None = None, skip = False):
        """
        Send action response to encoder.
        Called by RL environment to provide QP offsets.
        """
        self.action_queue.put(Action(
            skip=skip,
            offsets=action
        ))

    def wait_for_feedback(self) -> Feedback:
        """
        Wait for feedback from encoder.
        Called by RL environment to get encoding results.
        """
        return self.feedback_queue.get()

    def get_byte_usage_diff(self, picture_number: int) -> tuple[int, int]:
        """
        Get the byte usage difference for a specific frame compared to the first round.
        Returns the difference in bytes used for encoding the frame.
        Args:
            picture_number (int): The frame number to check.
        Returns:
            (int, int): A tuple containing the difference in bytes and the current size of the frame.
            positive difference means the frame is larger than in the first round,
            negative difference means the frame is smaller.
        """
        if (
            picture_number in self.first_round_byte_usage
            and picture_number in self.bytes_keeper
        ):
            first_round_size = self.first_round_byte_usage[picture_number]
            current_size = len(self.bytes_keeper.get(picture_number, b""))
            return (first_round_size - current_size, current_size)
        assert False, f"Frame {picture_number} not found in first round byte usage"
        
    def save_bitstream_to_file(self, output_path: str, interrupt: bool = False):
        """
        Save the bitstream to a file.
        If not interrupted, it will save current bitstream data to ivf file.
        If interrupted, it will save the previous training bytes to ivf file.
        
        If the desierd bitestream is not complete, nothing will be saved and a warning will be given
        Args:
            output_path (str): The path to save the bitstream file. Must end with .ivf.
            interrupt (bool): If True, save the previous training bytes instead of current.
        Raises:
            ValueError: If the output path does not end with .ivf.
        """
        if not output_path.endswith(".ivf"):
            raise ValueError("Output path must end with .ivf")

        # Save previous training bytes
        # To make the bitstream playable, prepend the IVF header and frame headers
        frames = list(self.previous_training_bytes_keeper.values())
        if not frames:
            bitstream_data = b""
        else:
            # Try to get width/height from the first frame using PyAV
            container = av.open(io.BytesIO(frames[0]))
            stream = next(iter(container.streams.video))
            width = stream.width
            height = stream.height
            container.close()
            bitstream_data = ivf_header(len(frames), width, height)
            for i, frame in enumerate(frames):
                bitstream_data += ivf_frame_header(frame, i)
                bitstream_data += frame

        if not bitstream_data:
            print("No bitstream data to save.")
            return

        with open(output_path, "wb") as f:
            f.write(bitstream_data)
        print(f"Bitstream saved to {output_path}")

def ivf_header(num_frames: int, width: int, height: int, fourcc: bytes = b'AV01'):
    header = b'DKIF'  # signature
    header += (0).to_bytes(2, 'little')  # version
    header += (32).to_bytes(2, 'little')  # header size
    header += fourcc  # fourcc
    header += width.to_bytes(2, 'little')
    header += height.to_bytes(2, 'little')
    header += (30).to_bytes(4, 'little')  # timebase denominator
    header += (1).to_bytes(4, 'little')   # timebase numerator
    header += num_frames.to_bytes(4, 'little')
    header += (0).to_bytes(4, 'little')  # unused
    return header

def ivf_frame_header(frame_bytes: bytes, pts: int):
    return len(frame_bytes).to_bytes(4, 'little') + pts.to_bytes(8, 'little')

def superblocks_from_dims(width: int, height: int, block_size: int = 64) -> int:
    """
    Return the number of super-blocks (block_size × block_size tiles)
    that cover a frame of size (width × height).

    Uses ceiling division so partially-covered edges count as full blocks.
    """
    if block_size <= 0:
        raise ValueError("block_size must be positive")

    blocks_w = (width  + block_size - 1) // block_size
    blocks_h = (height + block_size - 1) // block_size
    return blocks_w * blocks_h

def frame_dims_from_capture(cap: cv2.VideoCapture) -> tuple[int, int]:
    """
    Grab the first frame from an *open* cv2.VideoCapture and
    return (width, height).  Restores the original frame pointer.
    """
    if not cap.isOpened():
        raise ValueError("cv2.VideoCapture is not opened")

    pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
    ok, frame = cap.read()
    cap.set(cv2.CAP_PROP_POS_FRAMES, pos) # restore

    if not ok:
        raise ValueError("Could not read a frame from capture")

    height, width = frame.shape[:2]
    return width, height

def frame_dims_from_file(path: str | Path) -> tuple[int, int]:
    """
    Open the video at *path*, read its first frame, and return (width, height).
    Closes the capture automatically.
    """
    cap = cv2.VideoCapture(str(path))
    try:
        if not cap.isOpened():
            raise FileNotFoundError(f"Could not open video file: {path}")
        return frame_dims_from_capture(cap)
    finally:
        cap.release()

================
File: src/av1gym/pyencoder/bridge/cb_registration.cpp
================
#include "cb_registration.hpp"
#include "utils.hpp"
#include "pybridge.h"

namespace py = pybind11;

namespace pybridge {

Callback* g_callbacks[static_cast<int>(CallbackEnum::Count)] = {nullptr};

static int set_cb_ptr(CallbackEnum which, bool unset)
{
    switch (which) {
        case CallbackEnum::GetDeltaQOffset:
            get_deltaq_offset_cb = unset ? nullptr : get_deltaq_offset_trampoline;
            return 0;
        case CallbackEnum::RecvPictureFeedback:
            recv_picture_feedback_cb = unset ? nullptr : recv_picture_feedback_trampoline;
            return 0;
        case CallbackEnum::RecvPostEncodeStats:
            recv_postencode_feedback_cb = unset ? nullptr : recv_postencode_feedback_trampoline;
            return 0;
        default:
            return -1;
    }
}

int pybridge_set_cb(CallbackEnum which, py::object callable)
{
    Callback &slot = *g_callbacks[static_cast<int>(which)];

    if (callable.is_none()) {
        // Unset function
        slot.py_func = std::move(py::none());
    }
    else {
        // Validate and store new function
        py::function function =  pyutils::validate_callable(callable, slot.n_args);
        slot.py_func = std::move(function);
    }

    // Enable the C trampoline for the encoder
    return set_cb_ptr(which, callable.is_none());
}

} // namespace bridge

================
File: src/av1gym/pyencoder/bridge/cb_registration.hpp
================
#pragma once
#include <pybind11/pybind11.h>

namespace pybridge {

enum class CallbackEnum {
    GetDeltaQOffset,
    RecvPictureFeedback,
    RecvPostEncodeStats,
    Count
};

struct Callback {
    pybind11::object py_func;
    void            *c_trampoline;
    int              n_args;
};

extern Callback* g_callbacks[static_cast<int>(CallbackEnum::Count)];

int pybridge_set_cb(CallbackEnum which, pybind11::object callable);

} // namespace pybridge

================
File: src/av1gym/pyencoder/bridge/pybridge.cpp
================
#include "cb_registration.hpp"
#include "utils.hpp"
#include "pybridge.h"

#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

namespace py = pybind11;
using namespace pybridge;

get_deltaq_offset_cb_t     get_deltaq_offset_cb     = nullptr;
recv_picture_feedback_cb_t recv_picture_feedback_cb = nullptr;
recv_postencode_feedback_cb_t recv_postencode_feedback_cb = nullptr;

extern "C" void get_deltaq_offset_trampoline(SuperBlockInfo *sb_info_array, int *offset_array, uint32_t sb_count, int32_t frame_type, int32_t picture_number) {
    Callback &cb = *g_callbacks[static_cast<int>(CallbackEnum::GetDeltaQOffset)];
    if (cb.py_func.is_none())
        return;

    py::gil_scoped_acquire acquire;

    py::function fcn = pyutils::validate_callable(cb.py_func, cb.n_args);

    // Convert sb_infos to dictionary
    py::list sb_info_list = pyutils::to_pylist(sb_info_array, sb_count, [](const SuperBlockInfo &sb) {
        return py::dict(
            py::arg("sb_org_x") = sb.sb_org_x,
            py::arg("sb_org_y") = sb.sb_org_y,
            py::arg("sb_height") = sb.sb_height,
            py::arg("sb_width") = sb.sb_width,
            py::arg("sb_qindex") = sb.sb_qindex,
            py::arg("sb_x_mv") = sb.sb_x_mv,
            py::arg("sb_y_mv") = sb.sb_y_mv
        );
    });

    py::object ret = fcn.operator()(sb_info_list, frame_type, picture_number);

    if (!py::isinstance<py::list>(ret)) {
        throw py::type_error("Expected return value of type list however was " +
                             py::cast<std::string>(ret.get_type().attr("__name__")));
    }

    py::list qp_map = py::cast<py::list>(ret);
    if (qp_map.size() != sb_count) {
        throw py::value_error("Expected return value of type list with size " + std::to_string(sb_count) +
                              " however was of size " + std::to_string(qp_map.size()));
    }

    for (uint32_t i = 0; i < sb_count; ++i) {
        py::object item = qp_map[i];

        if (!py::isinstance<py::int_>(item)) {
            throw py::type_error("qp_map[" + std::to_string(i) + "] is not an int, got " +
                                 py::cast<std::string>(item.get_type().attr("__name__")));
        }

        offset_array[i] = item.cast<int>();
    }
}

extern "C" void recv_picture_feedback_trampoline(uint8_t *bitstream, uint32_t bitstream_size, uint32_t picture_number) {
    Callback &cb = *g_callbacks[static_cast<int>(CallbackEnum::RecvPictureFeedback)];
    if (cb.py_func.is_none())
        return;

    py::gil_scoped_acquire acquire;

    py::function fcn = pyutils::validate_callable(cb.py_func, cb.n_args);

    py::bytes py_bitstream = py::bytes(reinterpret_cast<char *>(bitstream), bitstream_size);

    fcn.operator()(py_bitstream, bitstream_size, picture_number);
}

extern "C" void recv_postencode_feedback_trampoline(uint32_t picture_number) {
    Callback &cb = *g_callbacks[static_cast<int>(CallbackEnum::RecvPostEncodeStats)];
    if (cb.py_func.is_none())
        return;

    py::gil_scoped_acquire acquire;

    py::function fcn = pyutils::validate_callable(cb.py_func, cb.n_args);


    // fcn.operator()(..., picture_number); //TODO
}

================
File: src/av1gym/pyencoder/bridge/pybridge.h
================
#ifndef PYBRIDGE_H_
#define PYBRIDGE_H_

#ifdef __cplusplus
extern "C" {
#endif

#include <stdint.h>
#include "../../../Source/API/EbSvtAv1Enc.h"

typedef void (*get_deltaq_offset_cb_t)(SuperBlockInfo *, int *, uint32_t, int32_t, int32_t);

typedef void (*recv_picture_feedback_cb_t)(uint8_t *, uint32_t, uint32_t);

typedef void (*recv_postencode_feedback_cb_t)(uint32_t);

extern get_deltaq_offset_cb_t get_deltaq_offset_cb;
extern recv_picture_feedback_cb_t recv_picture_feedback_cb;
extern recv_postencode_feedback_cb_t recv_postencode_feedback_cb;

void get_deltaq_offset_trampoline(SuperBlockInfo *, int *, uint32_t, int32_t, int32_t);

void recv_picture_feedback_trampoline(uint8_t *, uint32_t, uint32_t);

void recv_postencode_feedback_trampoline(uint32_t);//TODO

#ifdef __cplusplus
} /* extern "C" */
#endif
#endif /* PYBRIDGE_H_ */

================
File: src/av1gym/pyencoder/bridge/utils.cpp
================
#include "utils.hpp"

#include <pybind11/pybind11.h>
#include <pybind11/functional.h>
#include <Python.h>

#include <cassert>

namespace pyutils {

py::function validate_callable(const py::object &callable, int expected_n_args)
{
    assert(PyGILState_Check());

    if (!PyCallable_Check(callable.ptr())) {
        throw py::type_error("object must be callable");
    }

    // verify __code__ object
    py::object code_obj = callable.attr("__code__");
    if (!PyCode_Check(code_obj.ptr())) {
        throw py::type_error("callable has no valid __code__");
    }

    // Grab arg numbers
    PyCodeObject *co = reinterpret_cast<PyCodeObject *>(code_obj.ptr());
    const int nargs       = co->co_argcount + co->co_posonlyargcount;
    const bool has_vararg = (co->co_flags & CO_VARARGS)     != 0;
    const bool has_varkw  = (co->co_flags & CO_VARKEYWORDS) != 0;

    if (has_vararg || has_varkw) {
        throw py::type_error("callback may not use *args or **kwargs");
    }

    if (nargs != expected_n_args) {
        throw py::type_error(
            "callback must take exactly " + std::to_string(expected_n_args) + " positional arguments (got " + std::to_string(nargs) + ")"
        );
    }

    // Return as a function
    return py::reinterpret_borrow<py::function>(callable);
}

}

================
File: src/av1gym/pyencoder/bridge/utils.hpp
================
#pragma once
#include <pybind11/pybind11.h>

namespace py  = pybind11;

namespace pyutils {

py::function validate_callable(const py::object &callable, int expected_n_args);

template <typename T, typename Transform>
py::list to_pylist(const T *data, std::size_t n, Transform &&fn)
{
    if (!data)
        throw py::value_error("data pointer is null");

    py::list out(n);
    for (std::size_t i = 0; i < n; ++i)
        out[i] = fn(data[i]);
    return out;
}

} // namespace pybridge

================
File: src/av1gym/pyencoder/__init__.py
================
from ._binding import SuperBlockInfo, register_callbacks, run

================
File: src/av1gym/pyencoder/_binding.cpp
================
extern "C" {
    #define main app_main
    #include "../Source/App/app_main.c"
    #undef main
}

#ifndef SVT_ENABLE_USER_CALLBACKS
#define SVT_ENABLE_USER_CALLBACKS 1
#endif

#include <pybind11/pybind11.h>

#include "../bridge/utils.hpp"
#include "../bridge/cb_registration.hpp"
#include "../bridge/pybridge.h"

#include "../Source/Lib/Globals/enc_callbacks.h"
#include "../Source/API/EbSvtAv1Enc.h"

#include <vector>
#include <string>

namespace py = pybind11;
using namespace pybridge;

int init_callbacks();
void deinit_callbacks();

// run(argv: List[str]) -> None
static py::object run(py::list py_argv)
{
    const int argc = static_cast<int>(py_argv.size());

    // Keep string storage alive for the whole call.
    std::vector<std::string> storage;
    storage.reserve(argc);

    std::vector<char *> argv;
    argv.reserve(argc + 1);

    // Parse args to argv and argc list
    for (const py::handle &item : py_argv) {
        storage.emplace_back(py::cast<std::string>(item));
        argv.push_back(const_cast<char *>(storage.back().c_str()));
    }
    argv.push_back(nullptr);

    int rc = 0;
    {
        // Release the GIL while the encoder CLI runs.
        py::gil_scoped_release release;
        rc = app_main(argc, argv.data());
        py::gil_scoped_acquire acquire;
        deinit_callbacks();
    }

    if (rc != 0) {
        throw std::runtime_error(
            "SvtAv1EncApp returned non‑zero exit code " + std::to_string(rc));
    }

    return py::none();
}

// register_callbacks(get_deltaq_offset=None, picture_feedback=None, postencode_feedback=None) -> None
static py::object register_callbacks(py::object py_get_deltaq_offset = py::none(),
                                     py::object py_picture_feedback = py::none(),
                                     py::object py_post_encode_feedback = py::none())
{
    init_callbacks();

    // Store the user callables and hook up the C trampolines
    pybridge_set_cb(CallbackEnum::GetDeltaQOffset, py_get_deltaq_offset);
    pybridge_set_cb(CallbackEnum::RecvPictureFeedback, py_picture_feedback);
    pybridge_set_cb(CallbackEnum::RecvPostEncodeStats, py_post_encode_feedback);

    // Tell SVT‑AV1 about the trampolines
    static PluginCallbacks cbs;
    cbs.user_get_deltaq_offset = get_deltaq_offset_cb;
    cbs.user_picture_feedback = recv_picture_feedback_cb;
    cbs.user_postencode_feedback = recv_postencode_feedback_cb;

    if (svt_av1_enc_set_callbacks(&cbs) != EB_ErrorNone) {
        throw std::runtime_error("failed to set callbacks");
    }

    return py::none();
}

PYBIND11_MODULE(_av1_wrapper, m)
{
    m.doc() = "In‑process bindings for the SVT‑AV1 encoder CLI";

    m.def("run", &run,
          "Run the SVT‑AV1 encoder CLI in‑process.");

    m.def("register_callbacks", &register_callbacks,
          py::arg("get_deltaq_offset") = py::none(),
          py::arg("picture_feedback")  = py::none(),
          py::arg("postencode_feedback")  = py::none(),
          "Attach callbacks to the SVT‑AV1 encoder.");
}

int init_callbacks()
{
    for (int i = 0; i < static_cast<int>(CallbackEnum::Count); ++i) {
        g_callbacks[i] = new Callback{py::none(), nullptr, 0};
    }
    g_callbacks[0]->n_args = 3; // GetDeltaQOffset
    g_callbacks[1]->n_args = 3; // RecvPictureFeedback
    g_callbacks[2]->n_args = 1; // RecvPostEncodeStats //TODO
    return 0;
}

void deinit_callbacks()
{
    for (int i = 0; i < static_cast<int>(CallbackEnum::Count); ++i) {
        delete g_callbacks[i];
        g_callbacks[i] = nullptr;
    }
}

================
File: src/av1gym/pyencoder/_binding.py
================
from importlib import import_module as _imp
from typing import TypedDict

_av1_wrapper = _imp("av1gym.pyencoder._av1_wrapper", package=__name__)
_run = _av1_wrapper.run
_register = _av1_wrapper.register_callbacks

class SuperBlockInfo(TypedDict):
    sb_org_x: int
    sb_org_y: int
    sb_width: int
    sb_height: int
    sb_qindex: int
    sb_x_mv: int
    sb_y_mv: int

def run(**kwargs):
    argv = ["svtav1"]
    for key, val in kwargs.items():
        # If key starts with '-', use it directly as a flag
        if key.startswith("-"):
            flag = key
        else:
            # Determine short or long flag
            flag = f"-{key}" if len(key) == 1 else f"--{key.replace('_', '-')}"

        # Convert value to string appropriately
        if isinstance(val, bool):
            argv.extend([flag, "1" if val else "0"])
        else:
            argv.extend([flag, str(val)])

    _run(argv)

def register_callbacks(*, get_deltaq_offset=None, picture_feedback=None, postencode_feedback=None):
    _register(get_deltaq_offset, picture_feedback, postencode_feedback)

================
File: src/av1gym/__init__.py
================
pass

================
File: CMakeLists.txt
================
# Header‑only but still adds a target

if(${FROM_PIP_INSTALL}) 
    message(STATUS "Building for pip install")
    cmake_minimum_required(VERSION 3.20)
    project(av1_gym_bridge LANGUAGES C CXX ASM)

    # Global compile options
    set(CMAKE_C_STANDARD   11)
    set(CMAKE_CXX_STANDARD 17)
    set(CMAKE_CXX_STANDARD_REQUIRED ON)
    set(CMAKE_POSITION_INDEPENDENT_CODE ON)
    set(CMAKE_C_FLAGS "-O0 ${CMAKE_C_FLAGS}")
    set(CMAKE_CXX_FLAGS "-O0 ${CMAKE_CXX_FLAGS}")

    add_compile_definitions(SVT_ENABLE_USER_CALLBACKS)

    # Compile in the upstream SVT‑AV1 LIB
    set(BUILD_SHARED_LIBS OFF  CACHE BOOL "" FORCE)
    set(BUILD_APPS        OFF  CACHE BOOL "" FORCE)

    add_subdirectory(
        ${CMAKE_CURRENT_LIST_DIR}/..
        ${CMAKE_CURRENT_BINARY_DIR}/svtcore
        EXCLUDE_FROM_ALL)
endif()

find_package(Threads  REQUIRED)
find_package(Python3  REQUIRED COMPONENTS Interpreter Development)
find_package(pybind11 CONFIG REQUIRED) 

# Add Safe‑string library
file(GLOB SAFE_SRC "${CMAKE_CURRENT_LIST_DIR}/../third_party/safestringlib/*.c")
add_library(safeclib STATIC ${SAFE_SRC})
target_include_directories(safeclib PUBLIC
    "${CMAKE_CURRENT_LIST_DIR}/../third_party/safestringlib")

# Compole plugin callbacks
add_library(svtav1_plugin STATIC
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals/enc_callbacks.c")

target_include_directories(svtav1_plugin PRIVATE
    "${CMAKE_CURRENT_LIST_DIR}/.."
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API")

target_link_libraries(svtav1_plugin PRIVATE Threads::Threads)

# Re‑build Source/App/ as a static library, renaming main()
file(GLOB APP_SRC "${CMAKE_CURRENT_LIST_DIR}/../Source/App/*.c")
add_library(svtav1_app STATIC ${APP_SRC})

target_compile_definitions(svtav1_app PRIVATE main=svt_enc_app_main)
target_include_directories(svtav1_app PUBLIC
    "${CMAKE_CURRENT_LIST_DIR}/.."
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/App"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API")

target_link_libraries(svtav1_app PUBLIC Threads::Threads safeclib)

# Build the internal Python extension _av1_wrapper.{so,dylib,pyd}
set(BRIDGE_SRC
    src/av1gym/pyencoder/bridge/pybridge.cpp
    src/av1gym/pyencoder/bridge/cb_registration.cpp
    src/av1gym/pyencoder/bridge/utils.cpp)

set(PYENCODER_SRC
    src/av1gym/pyencoder/_binding.cpp
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals/rl_feedback.c")

add_library(_av1_wrapper MODULE
    ${BRIDGE_SRC}
    ${PYENCODER_SRC})

# On Windows a Python extension must be named *.pyd without the "lib" prefix.
if (WIN32)
    set_target_properties(_av1_wrapper PROPERTIES
        OUTPUT_NAME "_av1_wrapper"
        PREFIX      ""
        SUFFIX      ".pyd")
else()
    set_target_properties(_av1_wrapper PROPERTIES PREFIX "")
endif()

# Include paths
target_include_directories(_av1_wrapper PRIVATE
    "${CMAKE_CURRENT_LIST_DIR}/.."
    src/av1gym/pyencoder/bridge
    "${CMAKE_CURRENT_LIST_DIR}/../Source"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/App"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/API"
    "${CMAKE_CURRENT_LIST_DIR}/../Source/Lib/Globals"
    ${Python3_INCLUDE_DIRS})

# Link targets
target_link_libraries(_av1_wrapper
    PRIVATE
        pybind11::module
        svtav1_plugin
        svtav1_app
        SvtAv1Enc
        safeclib
        Python3::Python
        Threads::Threads
        m)                   # libm on *nix

# Installation path for scikit‑build‑core
if(${FROM_PIP_INSTALL})
    install(TARGETS _av1_wrapper
        LIBRARY DESTINATION av1gym/pyencoder
        RUNTIME DESTINATION av1gym/pyencoder)
else()
    install(TARGETS _av1_wrapper
        LIBRARY DESTINATION ${Python3_SITEARCH}/av1gym/pyencoder
        RUNTIME DESTINATION ${Python3_SITEARCH}/av1gym/pyencoder)
endif()

================
File: pyproject.toml
================
[project]
name = "av1-gym"
dynamic = ["version"]
authors = [{ name = "TL26" }]
readme = "README.md"
requires-python = ">=3.9"
dependencies = [
  "gymnasium>=0.29", 
  "numpy>=2.1.3", 
  "opencv-python>= 4.10", 
  "pybind11>=2.13.6", 
  "av>=14.4.0"
]

[build-system]
requires = [
  "scikit-build-core>=0.11",
  "setuptools_scm>=7",
  "pybind11_stubgen>=2"
]
build-backend = "scikit_build_core.build"

[tool.scikit-build]
wheel.packages = ["src/av1gym"]
install.strip = false

[tool.scikit-build.cmake]
define = { SVT_ENABLE_USER_CALLBACKS = "ON" }
args = [
    "-DCMAKE_BUILD_TYPE=Debug",
    "-DCMAKE_C_FLAGS=-g",
    "-DCMAKE_CXX_FLAGS=-g",
    "-DCMAKE_INSTALL_DO_STRIP=OFF",
    "-DFROM_PIP_INSTALL=1",
]

================
File: test.py
================
import av1gym.pyencoder as bridge

def get_deltaq_offset(sbs: list[bridge.SuperBlockInfo], frame_type: int, frame_number: int) -> list[int]:
    print("In python: ", len(sbs), frame_type, frame_number)
    return [0]*len(sbs)

def picture_feedback(a, b, c):
    pass

bridge.register_callbacks(get_deltaq_offset=get_deltaq_offset, picture_feedback=picture_feedback)

args = {
    "input": "../../playground/bus_cif.y4m",
    "pred_struct": 1,
    "rc": 2,
    "tbr": 100,
    "enable_stat_report": True
}

bridge.run(**args)



================================================================
End of Codebase
================================================================
